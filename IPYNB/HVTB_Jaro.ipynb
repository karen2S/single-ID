{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyjarowinkler import distance\n",
    "import Levenshtein\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Import and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_3804\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_3804\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_3804\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_3804\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_3804\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_3804\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_3804\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_3804\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_3804\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    r'D:\\Kuliah\\SEMESTER 8\\Skripsi\\Dataset\\Sheet1_SID_DATA_COBORR_PASSANGER_1124.csv',\n",
    "    r'D:\\Kuliah\\SEMESTER 8\\Skripsi\\Dataset\\Sheet2_SID_DATA_COBORR_PASSANGER_1124.csv',\n",
    "    r'D:\\Kuliah\\SEMESTER 8\\Skripsi\\Dataset\\Sheet3_SID_DATA_COBORR_PASSANGER_1124.csv'\n",
    "]\n",
    "dtype_params = {\n",
    "    'NO_AGGR': str,\n",
    "    'NO_NPWP': str,\n",
    "    'NO_KTP_KITAS': str,\n",
    "    'NO_KTP_COBORR': str\n",
    "}\n",
    "parse_dates_params = ['DT_GOLIVE_VALID', 'TGL_LAHIR', 'TGL_LAHIR_COBORR']\n",
    "\n",
    "df = pd.concat(\n",
    "    [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df[df['flag_PC'] == 'P']\n",
    "grouped_df = grouped_df.reset_index(drop=True)\n",
    "\n",
    "dataset = grouped_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANSING\n",
    "def clean_and_validation(df):\n",
    "    # Cleanse Nama Ibu Kandung\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['NAMA_IBU_KANDUNG'].apply(lambda x: str(x).upper() if pd.notna(x) else '')\n",
    "    \n",
    "    # Cleanse Tempat Lahir\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['TEMPAT_LAHIR'].apply(lambda x: str(x).upper() if pd.notna(x) else '')\n",
    "    # Remove 'CONVERTED', 'OTHERS', 'OTHER' and clean up single letter and repeated characters\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['cleaned_TEMPAT_LAHIR'].apply(lambda x: '' if x in ['CONVERTED', 'OTHERS', 'OTHER'] else x)\n",
    "    # Remove spaces in TEMPAT_LAHIR\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['cleaned_TEMPAT_LAHIR'].str.replace(\" \", \"\", regex=False)\n",
    "    # Check for single letter and repeated characters after cleaning spaces\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['cleaned_TEMPAT_LAHIR'].apply(lambda x: '' if re.match(r'^[A-Za-z]$', x) or re.match(r'^(.)\\1*$', x) else x)\n",
    "    \n",
    "    df['TGL_LAHIR'] = pd.to_datetime(df['TGL_LAHIR'], errors='coerce')\n",
    "\n",
    "    # 1. Remove prefixes\n",
    "    def cleanse_name(name):\n",
    "        if not isinstance(name, str):\n",
    "            return ''\n",
    "        \n",
    "        # List of known prefixes (expand as needed)\n",
    "        prefixes = [\"IR.\", \"IR\", \"DR.\", \"DR\", \"PROF.\", \"PROF\", \"DRS.\", \"DRS\", \"DRA.\", \"DRA\", \"DRG\", \"DRG.\", \"HJ\", \"HJ.\", \"ALM\", \"ALM.\", \"ALMH\", \"ALMH.\"]\n",
    "        \n",
    "        # Regex to match one or more prefixes followed by spaces, symbols, or the end of the string\n",
    "        prefix_pattern = r'^((?:\\b' + '|'.join(map(re.escape, prefixes)) + r'\\.?)\\s*)+[\\s\\W_]+'\n",
    "        \n",
    "        # Remove the prefixes only when followed by a space, symbol, or the end of the string\n",
    "        name = re.sub(prefix_pattern, '', name).strip()\n",
    "\n",
    "        # Remove underscores as symbols\n",
    "        name = name.replace('_', '')\n",
    "\n",
    "        return name\n",
    "\n",
    "    # Initialize cleaned_name column\n",
    "    df['cleaned_name'] = df['NAME_GOLIVE'].apply(cleanse_name)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(cleanse_name)\n",
    "\n",
    "    # 2. Clean Title\n",
    "    def create_pattern(title_list):\n",
    "        patterns = []\n",
    "        for title in title_list:\n",
    "            pattern = r'\\b' + r'\\s*'.join([re.escape(part) for part in title]) + r'\\b'\n",
    "            patterns.append(pattern)\n",
    "        return re.compile('|'.join(patterns), re.IGNORECASE)\n",
    "\n",
    "    # Title lists\n",
    "    title_4 = [\n",
    "        (\"S\", \"I\", \"K\", \"K\"), (\"S\", \"K\", \"P\", \"M\")\n",
    "    ]\n",
    "    title_3 = [\n",
    "        (\"S\", \"I\", \"A\"), (\"S\", \"TR\", \"AK\"), (\"S\", \"E\", \"AS\"), (\"S\", \"A\", \"B\"), \n",
    "        (\"S\", \"PD\", \"B\"), (\"S\", \"TR\", \"BNS\"), (\"S\", \"BIS\", \"DIG\"), (\"S\", \"K\", \"G\"), \n",
    "        (\"S\", \"FIL\", \"H\"), (\"S\", \"H\", \"H\"), (\"S\", \"K\", \"H\"), (\"S\", \"PD\", \"H\"), \n",
    "        (\"S\", \"SOS\", \"H\"), (\"S\", \"TR\", \"HAN\"), (\"S\", \"E\", \"I\"), (\"S\", \"FIL\", \"I\"), \n",
    "        (\"S\", \"H\", \"I\"), (\"S\", \"KOM\", \"I\"), (\"S\", \"PD\", \"I\"), (\"S\", \"SOS\", \"I\"), \n",
    "        (\"S\", \"HUB\", \"INT\"), (\"S\", \"TR\", \"IP\"), (\"S\", \"I\", \"K\"), (\"S\", \"TR\", \"K\"), \n",
    "        (\"S\", \"TR\", \"KEB\"), (\"S\", \"I\", \"KOM\"), (\"S\", \"TR\", \"KOM\"), (\"S\", \"K\", \"L\"), \n",
    "        (\"S\", \"K\", \"M\"), (\"S\", \"A\", \"N\"), (\"S\", \"A\", \"P\"), (\"S\", \"I\", \"P\"), \n",
    "        (\"S\", \"T\", \"P\"), (\"S\", \"ST\", \"PI\"), (\"S\", \"TR\", \"PI\"), (\"S\", \"I\", \"PTK\"), \n",
    "        (\"S\", \"PD\", \"SD\"), (\"S\", \"PD\", \"SI\"), (\"S\", \"TR\", \"SOS\"), (\"S\", \"E\", \"SY\"), \n",
    "        (\"S\", \"TR\", \"T\"), (\"S\", \"SI\", \"TH\"), (\"M\", \"B\", \"A\")\n",
    "    ]\n",
    "    title_2 = [\n",
    "        (\"S\", \"ADM\"), (\"S\", \"AG\"), (\"S\", \"AK\"), (\"S\", \"ANT\"), (\"S\", \"ARS\"),\n",
    "        (\"S\", \"DES\"), (\"S\", \"DS\"), (\"S\", \"E\"), (\"S\", \"FARM\"), (\"S\", \"FIL\"),\n",
    "        (\"S\", \"FT\"), (\"S\", \"GZ\"), (\"S\", \"H\"), (\"S\", \"HAN\"), (\"S\", \"HUM\"),\n",
    "        (\"S\", \"HUT\"), (\"S\", \"IIP\"), (\"S\", \"IK\"), (\"S\", \"IN\"), (\"S\", \"IP\"),\n",
    "        (\"S\", \"KEB\"), (\"S\", \"KED\"), (\"S\", \"KEL\"), (\"S\", \"KEP\"), (\"S\", \"KG\"),\n",
    "        (\"S\", \"KOM\"), (\"S\", \"LI\"), (\"S\", \"M\"), (\"S\", \"MB\"), (\"S\", \"P\"), (\"I\", \"R\"),\n",
    "        (\"S\", \"PAR\"), (\"S\", \"PD\"), (\"S\", \"PI\"), (\"S\", \"PN\"), (\"S\", \"PSI\"),\n",
    "        (\"S\", \"PT\"), (\"S\", \"PTK\"), (\"S\", \"PWK\"), (\"S\", \"S\"), (\"S\", \"SI\"),\n",
    "        (\"S\", \"SN\"), (\"S\", \"SOS\"), (\"S\", \"ST\"), (\"S\", \"STAT\"), (\"S\", \"STP\"),\n",
    "        (\"S\", \"SY\"), (\"S\", \"T\"), (\"S\", \"TH\"), (\"S\", \"TI\"), (\"M\", \"T\"), (\"S\", \"E\"), (\"PH\", \"D\"), (\"S\", \"AKTR\"),  (\"M\", \"E\")\n",
    "    ]\n",
    "\n",
    "    # Compile all patterns\n",
    "    pattern_4char = create_pattern(title_4)\n",
    "    pattern_3char = create_pattern(title_3)\n",
    "    pattern_2char = create_pattern(title_2)\n",
    "\n",
    "    def clean_name(name):\n",
    "        if not isinstance(name, str):  # Ensure input is a string\n",
    "            return ''\n",
    "\n",
    "        # Remove non-word characters and symbols\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    \n",
    "\n",
    "        # Remove numeric characters\n",
    "        name = ''.join([char for char in name if not char.isdigit()])\n",
    "\n",
    "        # Iteratively remove titles until no match is found\n",
    "        while True:\n",
    "            old_name = name\n",
    "            name = pattern_4char.sub('', name)\n",
    "            name = pattern_3char.sub('', name)\n",
    "            name = pattern_2char.sub('', name)\n",
    "            if name == old_name:  # Stop if no further changes\n",
    "                break\n",
    "\n",
    "        return name.strip()\n",
    "\n",
    "    # Apply name cleaning\n",
    "    df['cleaned_name'] = df['cleaned_name'].apply(clean_name)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(clean_name)\n",
    "    \n",
    "    # remove first letter in name\n",
    "    #df['cleaned_name'] = df['cleaned_name'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()[0]) == 1 else x)\n",
    "    #df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()[0]) == 1 else x)\n",
    "    df['cleaned_name'] = df['cleaned_name'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()) > 1 and len(x.split()[0]) == 1 else x)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()) > 1 and len(x.split()[0]) == 1 else x)\n",
    "\n",
    "    # 3. Clean NO_KTP_KITAS column\n",
    "    df['cleaned_no_ktp'] = df['NO_KTP_KITAS'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "    # Function to clean the KTP values\n",
    "    def clean_no_ktp(ktp):\n",
    "        # If length is 1 or the value consists of repeating characters (e.g., \"1111111111111111\", \"XXXX\", etc.)\n",
    "        if len(ktp) == 1 or len(set(ktp)) == 1:  \n",
    "            return None  # Return None for invalid cases\n",
    "        return ktp\n",
    "\n",
    "    # Apply the cleaning function to the 'NO_KTP_KITAS' column\n",
    "    df['cleaned_no_ktp'] = df['cleaned_no_ktp'].apply(clean_no_ktp)\n",
    "\n",
    "    # 4. Clean NO_NPWP column\n",
    "    df['cleaned_no_npwp'] = df['NO_NPWP'].apply(lambda x: ''.join(filter(str.isdigit, str(x))))\n",
    "\n",
    "\n",
    "    # 5. Clean Address\n",
    "    substitutions = [\n",
    "        (r'\\b(JLN\\s|JLN\\.|JALAN\\s|JALAN\\.|JL\\.|JLH\\s|JLH\\.)\\s?', 'JL '),\n",
    "        (r'\\b(GANG\\s|GANG\\.|GG\\.)\\s?', 'GG '),\n",
    "        (r'\\b(PONDOK\\s|PONDOK\\.|PD\\.)\\s?', 'PD '),\n",
    "        (r'\\b(PERUMAHAN\\s|PERUMAHAN\\.|PERUM\\s|PERUM\\.|PRM\\.|PERUMNAS\\s|PERUMNAS\\.)\\s?', 'PRM '),\n",
    "        (r'\\b(DESA\\s|DESA\\.|DUSUN\\.|DUSUN\\s|DS\\.|DSN\\.|DSN\\s)\\s?', 'DS '),\n",
    "        (r'\\b(KP\\.|KPG\\.|KPG\\s|KAMPUNG\\s|KAMPUNG\\.|KAMP\\s|KAMP\\.|KMP\\s|KMP\\.)\\s?', 'KP '),\n",
    "        (r'\\b(APARTEMEN\\.|APARTEMEN\\s|APART\\.|APART\\s|APARTMENT\\.|APARTMENT\\s|APARTEMENT\\.|APARTEMENT\\s|AP\\s|AP\\.|APT\\.)\\s?', 'APT '),\n",
    "        (r'\\b(KOMPLEKS\\s|KOMPLEKS\\.|KOMPLEK\\s|KOMPLEK\\.|KOMPL\\s|KOMPL\\.|KOMP\\s|KOMP\\.|KOM\\.)\\s?', 'KOM '),\n",
    "        (r'\\b(LINGKUNGAN\\s|LINGKUNGAN\\.|LINGK\\s|LINGK\\.|LKG\\.)\\s?', 'LKG '),\n",
    "        (r'\\b(TAMAN\\s|TAMAN\\.|TMN\\s|TMN\\.|TM\\.)\\s?', 'TM '),\n",
    "        (r'\\b(BLOK\\.|BLOK\\s|BLK\\.|BLK\\s|BL\\.)\\s?', 'BL '),\n",
    "        (r'\\b(VILLA\\s|VILLA\\.|VILA\\s|VILA\\.|VIL\\.)\\s?', 'VIL '),\n",
    "        (r'\\b(GRIYA\\s|GRIYA\\.|GRY\\.)\\s?', 'GRY '),\n",
    "        (r'\\b(ASRAMA\\s|ASRAMA\\.|ASR\\.)\\s?', 'ASR '),\n",
    "        (r'\\b(TANJUNG\\s|TANJUNG\\.|TJ\\.)\\s?', 'TJ ')\n",
    "    ]\n",
    "\n",
    "    # Known abbreviations with potential spacing issues\n",
    "    long_abbreviations = [\"APARTEMEN\", \"APARTEMENT\", \"APARTMENT\", \"TANJUNG\", \"LINGKUNGAN\", \"ASRAMA\", \n",
    "                        \"KOMPLEKS\", \"PERUMAHAN\", \"KAMPUNG\", \"PERUMNAS\", \"PONDOK\"]\n",
    "\n",
    "    # Function to add space between compound words\n",
    "    def add_space_between_compound_words(address):\n",
    "        for abbr in long_abbreviations:\n",
    "            address = re.sub(rf'({abbr})([A-Z])', r'\\1 \\2', address)\n",
    "        return address\n",
    "\n",
    "    # Function to apply all substitutions globally\n",
    "    def update_address(address):\n",
    "        if not isinstance(address, str):\n",
    "            return ''  # Handle non-string inputs gracefully\n",
    "        \n",
    "        # Correct compound abbreviations\n",
    "        address = add_space_between_compound_words(address)\n",
    "        \n",
    "        # Apply all substitution patterns globally\n",
    "        for pattern, replacement in substitutions:\n",
    "            address = re.sub(pattern, replacement, address, flags=re.IGNORECASE)\n",
    "        \n",
    "        return address\n",
    "\n",
    "    df['cleaned_alamat'] = df['ALMT_RUMAH'].apply(update_address)\n",
    "    \n",
    "    abbreviations_to_remove = [\n",
    "        \"JL\", \"GG\", \"PD\", \"PRM\", \"DS\", \"KP\", \"APT\", \"KOM\", \"LKG\", \"TM\", \"BL\", \"VIL\", \"GRY\", \"ASR\", \"TJ\"\n",
    "    ]\n",
    "\n",
    "    # Regex pattern for matching Roman numerals\n",
    "    roman_numeral_pattern = r'\\b(M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))\\b'\n",
    "\n",
    "    # Regex pattern to remove \"NO\" or \"NOMOR\" followed by any number and optional spaces\n",
    "    no_nomor_pattern = r'\\b(NO|NOMOR)\\s?\\d*\\b'\n",
    "\n",
    "    # Function to remove abbreviations, numbers, Roman numerals, and \"NO\" or \"NOMOR\" followed by numbers from address\n",
    "    def deep_clean_address(address):\n",
    "        if not isinstance(address, str):\n",
    "            return ''  # Handle non-string inputs gracefully\n",
    "        \n",
    "        # Remove abbreviations\n",
    "        for abbr in abbreviations_to_remove:\n",
    "            address = re.sub(rf'\\b{abbr}\\b\\s?', '', address, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove all numbers\n",
    "        address = re.sub(r'\\d+', '', address)\n",
    "        \n",
    "        # Remove Roman numerals using regex\n",
    "        address = re.sub(roman_numeral_pattern, '', address)\n",
    "        \n",
    "        # Remove \"NO\" or \"NOMOR\" followed by numbers\n",
    "        address = re.sub(no_nomor_pattern, '', address)\n",
    "        \n",
    "        address = re.sub(r'[^a-zA-Z0-9\\s]', '', address)\n",
    "        \n",
    "        address = re.sub(r'\\s+', ' ', address).strip()\n",
    "        \n",
    "        return address\n",
    "    df['deep_clean_address'] = df['cleaned_alamat'].apply(deep_clean_address)\n",
    "\n",
    "    # Final fallback if name is empty after cleaning\n",
    "    df['cleaned_name'] = df.apply(lambda row: row['cleaned_name'] if row['cleaned_name'] != '' else row['NAME_GOLIVE'], axis=1)    \n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df.apply(lambda row: row['cleaned_NAMA_IBU_KANDUNG'] if row['cleaned_NAMA_IBU_KANDUNG'] != '' else row['NAMA_IBU_KANDUNG'], axis=1) \n",
    "    \n",
    "    # Remove 'BUNDA', 'UMI', 'IBU', 'BU', 'NY'\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(lambda x: '' if x in ['BUNDA', 'UMI', 'IBU', 'BU', 'NY', 'NYONYA'] else x)\n",
    "\n",
    "    # Remove rows with a single letter or repeated characters for cleaned_NAMA_IBU_KANDUNG\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(\n",
    "    lambda x: '' if isinstance(x, str) and (re.match(r'^[A-Za-z]$', x) or re.match(r'^(.)\\1*$', x)) else x)\n",
    "\n",
    "    def remove_extra_spaces(text):\n",
    "        # Check if the input is a string or not\n",
    "        if not isinstance(text, str):\n",
    "            return ''  # Return an empty string for non-strings (e.g., NaN, float)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    df['cleaned_name'] = df['cleaned_name'].apply(remove_extra_spaces)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(remove_extra_spaces)\n",
    "    df['cleaned_alamat'] = df['cleaned_alamat'].apply(remove_extra_spaces)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the clean_and_validation function\n",
    "dataset = clean_and_validation(dataset)\n",
    "backup_dataset = dataset.copy()\n",
    "dataset.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANSING\n",
    "def clean_and_validation(df):\n",
    "    # Cleanse Nama Ibu Kandung\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['NAMA_IBU_KANDUNG'].apply(lambda x: str(x).upper() if pd.notna(x) else '')\n",
    "    \n",
    "    # Cleanse Tempat Lahir\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['TEMPAT_LAHIR'].apply(lambda x: str(x).upper() if pd.notna(x) else '')\n",
    "    # Remove 'CONVERTED', 'OTHERS', 'OTHER' and clean up single letter and repeated characters\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['cleaned_TEMPAT_LAHIR'].apply(lambda x: '' if x in ['CONVERTED', 'OTHERS', 'OTHER'] else x)\n",
    "    # Remove spaces in TEMPAT_LAHIR\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['cleaned_TEMPAT_LAHIR'].str.replace(\" \", \"\", regex=False)\n",
    "    # Check for single letter and repeated characters after cleaning spaces\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['cleaned_TEMPAT_LAHIR'].apply(lambda x: '' if re.match(r'^[A-Za-z]$', x) or re.match(r'^(.)\\1*$', x) else x)\n",
    "    \n",
    "    df['TGL_LAHIR'] = pd.to_datetime(df['TGL_LAHIR'], errors='coerce')\n",
    "\n",
    "    # 1. Remove prefixes\n",
    "    def cleanse_name(name):\n",
    "        if not isinstance(name, str):\n",
    "            return ''\n",
    "        \n",
    "        # List of known prefixes (expand as needed)\n",
    "        prefixes = [\"IR.\", \"IR\", \"DR.\", \"DR\", \"PROF.\", \"PROF\", \"DRS.\", \"DRS\", \"DRA.\", \"DRA\", \"DRG\", \"DRG.\", \"HJ\", \"HJ.\", \"ALM\", \"ALM.\", \"ALMH\", \"ALMH.\"]\n",
    "        \n",
    "        # Regex to match one or more prefixes followed by spaces, symbols, or the end of the string\n",
    "        prefix_pattern = r'^((?:\\b' + '|'.join(map(re.escape, prefixes)) + r'\\.?)\\s*)+[\\s\\W_]+'\n",
    "        \n",
    "        # Remove the prefixes only when followed by a space, symbol, or the end of the string\n",
    "        name = re.sub(prefix_pattern, '', name).strip()\n",
    "\n",
    "        # Remove underscores as symbols\n",
    "        name = name.replace('_', '')\n",
    "\n",
    "        return name\n",
    "\n",
    "    # Initialize cleaned_name column\n",
    "    df['cleaned_name'] = df['NAME_GOLIVE'].apply(cleanse_name)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(cleanse_name)\n",
    "\n",
    "    # 2. Clean Title\n",
    "    def create_pattern(title_list):\n",
    "        patterns = []\n",
    "        for title in title_list:\n",
    "            pattern = r'\\b' + r'\\s*'.join([re.escape(part) for part in title]) + r'\\b'\n",
    "            patterns.append(pattern)\n",
    "        return re.compile('|'.join(patterns), re.IGNORECASE)\n",
    "\n",
    "    # Title lists\n",
    "    title_4 = [\n",
    "        (\"S\", \"I\", \"K\", \"K\"), (\"S\", \"K\", \"P\", \"M\")\n",
    "    ]\n",
    "    title_3 = [\n",
    "        (\"S\", \"I\", \"A\"), (\"S\", \"TR\", \"AK\"), (\"S\", \"E\", \"AS\"), (\"S\", \"A\", \"B\"), \n",
    "        (\"S\", \"PD\", \"B\"), (\"S\", \"TR\", \"BNS\"), (\"S\", \"BIS\", \"DIG\"), (\"S\", \"K\", \"G\"), \n",
    "        (\"S\", \"FIL\", \"H\"), (\"S\", \"H\", \"H\"), (\"S\", \"K\", \"H\"), (\"S\", \"PD\", \"H\"), \n",
    "        (\"S\", \"SOS\", \"H\"), (\"S\", \"TR\", \"HAN\"), (\"S\", \"E\", \"I\"), (\"S\", \"FIL\", \"I\"), \n",
    "        (\"S\", \"H\", \"I\"), (\"S\", \"KOM\", \"I\"), (\"S\", \"PD\", \"I\"), (\"S\", \"SOS\", \"I\"), \n",
    "        (\"S\", \"HUB\", \"INT\"), (\"S\", \"TR\", \"IP\"), (\"S\", \"I\", \"K\"), (\"S\", \"TR\", \"K\"), \n",
    "        (\"S\", \"TR\", \"KEB\"), (\"S\", \"I\", \"KOM\"), (\"S\", \"TR\", \"KOM\"), (\"S\", \"K\", \"L\"), \n",
    "        (\"S\", \"K\", \"M\"), (\"S\", \"A\", \"N\"), (\"S\", \"A\", \"P\"), (\"S\", \"I\", \"P\"), \n",
    "        (\"S\", \"T\", \"P\"), (\"S\", \"ST\", \"PI\"), (\"S\", \"TR\", \"PI\"), (\"S\", \"I\", \"PTK\"), \n",
    "        (\"S\", \"PD\", \"SD\"), (\"S\", \"PD\", \"SI\"), (\"S\", \"TR\", \"SOS\"), (\"S\", \"E\", \"SY\"), \n",
    "        (\"S\", \"TR\", \"T\"), (\"S\", \"SI\", \"TH\"), (\"M\", \"B\", \"A\")\n",
    "    ]\n",
    "    title_2 = [\n",
    "        (\"S\", \"ADM\"), (\"S\", \"AG\"), (\"S\", \"AK\"), (\"S\", \"ANT\"), (\"S\", \"ARS\"),\n",
    "        (\"S\", \"DES\"), (\"S\", \"DS\"), (\"S\", \"E\"), (\"S\", \"FARM\"), (\"S\", \"FIL\"),\n",
    "        (\"S\", \"FT\"), (\"S\", \"GZ\"), (\"S\", \"H\"), (\"S\", \"HAN\"), (\"S\", \"HUM\"),\n",
    "        (\"S\", \"HUT\"), (\"S\", \"IIP\"), (\"S\", \"IK\"), (\"S\", \"IN\"), (\"S\", \"IP\"),\n",
    "        (\"S\", \"KEB\"), (\"S\", \"KED\"), (\"S\", \"KEL\"), (\"S\", \"KEP\"), (\"S\", \"KG\"),\n",
    "        (\"S\", \"KOM\"), (\"S\", \"LI\"), (\"S\", \"M\"), (\"S\", \"MB\"), (\"S\", \"P\"), (\"I\", \"R\"),\n",
    "        (\"S\", \"PAR\"), (\"S\", \"PD\"), (\"S\", \"PI\"), (\"S\", \"PN\"), (\"S\", \"PSI\"),\n",
    "        (\"S\", \"PT\"), (\"S\", \"PTK\"), (\"S\", \"PWK\"), (\"S\", \"S\"), (\"S\", \"SI\"),\n",
    "        (\"S\", \"SN\"), (\"S\", \"SOS\"), (\"S\", \"ST\"), (\"S\", \"STAT\"), (\"S\", \"STP\"),\n",
    "        (\"S\", \"SY\"), (\"S\", \"T\"), (\"S\", \"TH\"), (\"S\", \"TI\"), (\"M\", \"T\"), (\"S\", \"E\"), (\"PH\", \"D\"), (\"S\", \"AKTR\"),  (\"M\", \"E\")\n",
    "    ]\n",
    "\n",
    "    # Compile all patterns\n",
    "    pattern_4char = create_pattern(title_4)\n",
    "    pattern_3char = create_pattern(title_3)\n",
    "    pattern_2char = create_pattern(title_2)\n",
    "\n",
    "    def clean_name(name):\n",
    "        if not isinstance(name, str):  # Ensure input is a string\n",
    "            return ''\n",
    "\n",
    "        # Remove non-word characters and symbols\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    \n",
    "\n",
    "        # Remove numeric characters\n",
    "        name = ''.join([char for char in name if not char.isdigit()])\n",
    "\n",
    "        # Iteratively remove titles until no match is found\n",
    "        while True:\n",
    "            old_name = name\n",
    "            name = pattern_4char.sub('', name)\n",
    "            name = pattern_3char.sub('', name)\n",
    "            name = pattern_2char.sub('', name)\n",
    "            if name == old_name:  # Stop if no further changes\n",
    "                break\n",
    "\n",
    "        return name.strip()\n",
    "\n",
    "    # Apply name cleaning\n",
    "    df['cleaned_name'] = df['cleaned_name'].apply(clean_name)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(clean_name)\n",
    "    \n",
    "    # remove first letter in name\n",
    "    #df['cleaned_name'] = df['cleaned_name'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()[0]) == 1 else x)\n",
    "    # df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()[0]) == 1 else x)\n",
    "    # df['cleaned_name'] = df['cleaned_name'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()) > 1 and len(x.split()[0]) == 1 else x)\n",
    "    # df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()) > 1 and len(x.split()[0]) == 1 else x)\n",
    "\n",
    "    # 3. Clean NO_KTP_KITAS column\n",
    "    df['cleaned_no_ktp'] = df['NO_KTP_KITAS'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "    # Function to clean the KTP values\n",
    "    def clean_no_ktp(ktp):\n",
    "        # If length is 1 or the value consists of repeating characters (e.g., \"1111111111111111\", \"XXXX\", etc.)\n",
    "        if len(ktp) == 1 or len(set(ktp)) == 1:  \n",
    "            return None  # Return None for invalid cases\n",
    "        return ktp\n",
    "\n",
    "    # Apply the cleaning function to the 'NO_KTP_KITAS' column\n",
    "    df['cleaned_no_ktp'] = df['cleaned_no_ktp'].apply(clean_no_ktp)\n",
    "\n",
    "    # 4. Clean NO_NPWP column\n",
    "    df['cleaned_no_npwp'] = df['NO_NPWP'].apply(lambda x: ''.join(filter(str.isdigit, str(x))))\n",
    "\n",
    "    # 5. Clean Address\n",
    "    substitutions = [\n",
    "        (r'\\b(JLN\\s|JLN\\.|JALAN\\s|JALAN\\.|JL\\.|JLH\\s|JLH\\.)\\s?', 'JL '),\n",
    "        (r'\\b(GANG\\s|GANG\\.|GG\\.)\\s?', 'GG '),\n",
    "        (r'\\b(PONDOK\\s|PONDOK\\.|PD\\.)\\s?', 'PD '),\n",
    "        (r'\\b(PERUMAHAN\\s|PERUMAHAN\\.|PERUM\\s|PERUM\\.|PRM\\.|PERUMNAS\\s|PERUMNAS\\.)\\s?', 'PRM '),\n",
    "        (r'\\b(DESA\\s|DESA\\.|DUSUN\\.|DUSUN\\s|DS\\.|DSN\\.|DSN\\s)\\s?', 'DS '),\n",
    "        (r'\\b(KP\\.|KPG\\.|KPG\\s|KAMPUNG\\s|KAMPUNG\\.|KAMP\\s|KAMP\\.|KMP\\s|KMP\\.)\\s?', 'KP '),\n",
    "        (r'\\b(APARTEMEN\\.|APARTEMEN\\s|APART\\.|APART\\s|APARTMENT\\.|APARTMENT\\s|APARTEMENT\\.|APARTEMENT\\s|AP\\s|AP\\.|APT\\.)\\s?', 'APT '),\n",
    "        (r'\\b(KOMPLEKS\\s|KOMPLEKS\\.|KOMPLEK\\s|KOMPLEK\\.|KOMPL\\s|KOMPL\\.|KOMP\\s|KOMP\\.|KOM\\.)\\s?', 'KOM '),\n",
    "        (r'\\b(LINGKUNGAN\\s|LINGKUNGAN\\.|LINGK\\s|LINGK\\.|LKG\\.)\\s?', 'LKG '),\n",
    "        (r'\\b(TAMAN\\s|TAMAN\\.|TMN\\s|TMN\\.|TM\\.)\\s?', 'TM '),\n",
    "        (r'\\b(BLOK\\.|BLOK\\s|BLK\\.|BLK\\s|BL\\.)\\s?', 'BL '),\n",
    "        (r'\\b(VILLA\\s|VILLA\\.|VILA\\s|VILA\\.|VIL\\.)\\s?', 'VIL '),\n",
    "        (r'\\b(GRIYA\\s|GRIYA\\.|GRY\\.)\\s?', 'GRY '),\n",
    "        (r'\\b(ASRAMA\\s|ASRAMA\\.|ASR\\.)\\s?', 'ASR '),\n",
    "        (r'\\b(TANJUNG\\s|TANJUNG\\.|TJ\\.)\\s?', 'TJ ')\n",
    "    ]\n",
    "\n",
    "    # Known abbreviations with potential spacing issues\n",
    "    long_abbreviations = [\"APARTEMEN\", \"APARTEMENT\", \"APARTMENT\", \"TANJUNG\", \"LINGKUNGAN\", \"ASRAMA\", \n",
    "                        \"KOMPLEKS\", \"PERUMAHAN\", \"KAMPUNG\", \"PERUMNAS\", \"PONDOK\"]\n",
    "\n",
    "    # Function to add space between compound words\n",
    "    def add_space_between_compound_words(address):\n",
    "        for abbr in long_abbreviations:\n",
    "            address = re.sub(rf'({abbr})([A-Z])', r'\\1 \\2', address)\n",
    "        return address\n",
    "\n",
    "    # Function to apply all substitutions globally\n",
    "    def update_address(address):\n",
    "        if not isinstance(address, str):\n",
    "            return ''  # Handle non-string inputs gracefully\n",
    "        \n",
    "        # Correct compound abbreviations\n",
    "        address = add_space_between_compound_words(address)\n",
    "        \n",
    "        # Apply all substitution patterns globally\n",
    "        for pattern, replacement in substitutions:\n",
    "            address = re.sub(pattern, replacement, address, flags=re.IGNORECASE)\n",
    "        \n",
    "        return address\n",
    "\n",
    "    df['cleaned_alamat'] = df['ALMT_RUMAH'].apply(update_address)\n",
    "    \n",
    "    abbreviations_to_remove = [\n",
    "        \"JL\", \"GG\", \"PD\", \"PRM\", \"DS\", \"KP\", \"APT\", \"KOM\", \"LKG\", \"TM\", \"BL\", \"VIL\", \"GRY\", \"ASR\", \"TJ\"\n",
    "    ]\n",
    "\n",
    "    # Regex pattern for matching Roman numerals\n",
    "    roman_numeral_pattern = r'\\b(M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))\\b'\n",
    "\n",
    "    # Regex pattern to remove \"NO\" or \"NOMOR\" followed by any number and optional spaces\n",
    "    no_nomor_pattern = r'\\b(NO|NOMOR)\\s?\\d*\\b'\n",
    "\n",
    "    # Function to remove abbreviations, numbers, Roman numerals, and \"NO\" or \"NOMOR\" followed by numbers from address\n",
    "    def deep_clean_address(address):\n",
    "        if not isinstance(address, str):\n",
    "            return ''  # Handle non-string inputs gracefully\n",
    "        \n",
    "        # Remove abbreviations\n",
    "        for abbr in abbreviations_to_remove:\n",
    "            address = re.sub(rf'\\b{abbr}\\b\\s?', '', address, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove all numbers\n",
    "        address = re.sub(r'\\d+', '', address)\n",
    "        \n",
    "        # Remove Roman numerals using regex\n",
    "        address = re.sub(roman_numeral_pattern, '', address)\n",
    "        \n",
    "        # Remove \"NO\" or \"NOMOR\" followed by numbers\n",
    "        address = re.sub(no_nomor_pattern, '', address)\n",
    "        \n",
    "        address = re.sub(r'[^a-zA-Z0-9\\s]', '', address)\n",
    "        \n",
    "        address = re.sub(r'\\s+', ' ', address).strip()\n",
    "        \n",
    "        return address\n",
    "    df['deep_clean_address'] = df['cleaned_alamat'].apply(deep_clean_address)\n",
    "\n",
    "    # Final fallback if name is empty after cleaning\n",
    "    df['cleaned_name'] = df.apply(lambda row: row['cleaned_name'] if row['cleaned_name'] != '' else row['NAME_GOLIVE'], axis=1)    \n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df.apply(lambda row: row['cleaned_NAMA_IBU_KANDUNG'] if row['cleaned_NAMA_IBU_KANDUNG'] != '' else row['NAMA_IBU_KANDUNG'], axis=1) \n",
    "    \n",
    "    # Remove 'BUNDA', 'UMI', 'IBU', 'BU', 'NY'\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(lambda x: '' if x in ['BUNDA', 'UMI', 'IBU', 'BU', 'NY', 'NYONYA'] else x)\n",
    "\n",
    "    # Remove rows with a single letter or repeated characters for cleaned_NAMA_IBU_KANDUNG\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(\n",
    "    lambda x: '' if isinstance(x, str) and (re.match(r'^[A-Za-z]$', x) or re.match(r'^(.)\\1*$', x)) else x)\n",
    "\n",
    "    def remove_extra_spaces(text):\n",
    "        # Check if the input is a string or not\n",
    "        if not isinstance(text, str):\n",
    "            return ''  # Return an empty string for non-strings (e.g., NaN, float)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    df['cleaned_name'] = df['cleaned_name'].apply(remove_extra_spaces)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(remove_extra_spaces)\n",
    "    df['cleaned_alamat'] = df['cleaned_alamat'].apply(remove_extra_spaces)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the clean_and_validation function\n",
    "dataset = clean_and_validation(dataset)\n",
    "backup_dataset = dataset.copy()\n",
    "dataset.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data_100000 = pd.read_excel(r'D:\\Kuliah\\SEMESTER 8\\Skripsi\\Dataset\\sampled_data_100000.xlsx')\n",
    "sampled_data_100 = pd.read_excel(r'D:\\Kuliah\\SEMESTER 8\\Skripsi\\Dataset\\sampled_data_100.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High Value Token Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_dict = defaultdict(list)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for idx, row in dataset.iterrows():\n",
    "    # Get the cleaned_name\n",
    "    name = row['cleaned_name']\n",
    "    if not isinstance(name, str) or not name.strip():\n",
    "        continue  # Skip rows with empty or invalid names\n",
    "    \n",
    "    # Split the name into tokens\n",
    "    tokens = name.split() \n",
    "    for token in tokens:\n",
    "        if name not in tokens_dict[token]:  # Avoid duplicates\n",
    "            tokens_dict[token].append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaro-Winkler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 826.68 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokens = [name.split() for name in dataset['cleaned_name']]\n",
    "all_tokens = [token for sublist in tokens for token in sublist]\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "final_blocks = defaultdict(list)\n",
    "similarity_list = defaultdict(list)\n",
    "\n",
    "start_time = time.time()\n",
    "for compared_name in sampled_data_100000['cleaned_name']:\n",
    "    compared_name = \" \".join(sorted(compared_name.split()))\n",
    "    name_tokens = compared_name.split()\n",
    "    least_frequent_token = min(name_tokens, key=lambda token: token_counts[token])\n",
    "    group = []\n",
    "\n",
    "    for name in tokens_dict[least_frequent_token]:\n",
    "        name = \" \".join(sorted(name.split()))\n",
    "        similarity = distance.get_jaro_distance(compared_name, name)\n",
    "        threshold = 0.75 * min(len(compared_name),len(name)) / max(len(compared_name), len(name))\n",
    "        if similarity > threshold:\n",
    "            group.append(name)\n",
    "            similarity_list[compared_name].append(similarity)\n",
    "        final_blocks[compared_name] = group  \n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name & Address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_dict = defaultdict(list)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for idx, row in dataset.iterrows():\n",
    "    # Get the cleaned_name and deep_clean_address\n",
    "    name = row['cleaned_name']\n",
    "    address = row['deep_clean_address']\n",
    "    \n",
    "    # Skip rows with invalid names or addresses\n",
    "    if not isinstance(name, str) or not name.strip() or not isinstance(address, str) or not address.strip():\n",
    "        continue\n",
    "    \n",
    "    # Split the address into tokens\n",
    "    address_tokens = address.split()\n",
    "    \n",
    "    # Store tuples (name, address) in address_dict\n",
    "    for token in address_tokens:\n",
    "        if (name, address) not in address_dict[token]:\n",
    "            address_dict[token].append((name, address))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = defaultdict(list)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for idx, row in dataset.iterrows():\n",
    "    # Get the cleaned_name\n",
    "    name = row['cleaned_name']\n",
    "    if not isinstance(name, str) or not name.strip():\n",
    "        continue  # Skip rows with empty or invalid names\n",
    "    \n",
    "    # Split the name into tokens\n",
    "    tokens = name.split() \n",
    "    for token in tokens:\n",
    "        if name not in name_dict[token]:  # Avoid duplicates\n",
    "            name_dict[token].append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaro-Winkler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 637.21 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset['deep_clean_address'] = dataset['deep_clean_address'].fillna(dataset['ALMT_RUMAH']).astype(str)\n",
    "sampled_data_100000['deep_clean_address'] = sampled_data_100000['deep_clean_address'].fillna(sampled_data_100000['ALMT_RUMAH']).astype(str)\n",
    "\n",
    "# Tokenizing addresses\n",
    "tokens_address = [str(address).split() for address in dataset['deep_clean_address']]\n",
    "all_tokens_address = [token for sublist in tokens_address for token in sublist]\n",
    "token_counts_address = Counter(all_tokens_address)\n",
    "\n",
    "# Tokenizing names\n",
    "tokens_name = [name.split() for name in dataset['cleaned_name']]\n",
    "all_tokens_name = [token for sublist in tokens_name for token in sublist]\n",
    "token_counts_name = Counter(all_tokens_name)\n",
    "\n",
    "# Initialize final structures\n",
    "final_blocks = defaultdict(list)\n",
    "similarity_list = defaultdict(list)\n",
    "\n",
    "start_time = time.time()\n",
    "# Iterate through the rows of the dataset\n",
    "for index, row in sampled_data_100000.iterrows():\n",
    "    compared_address = row['deep_clean_address']\n",
    "    compared_name = row['cleaned_name']\n",
    "    compared_name = \" \".join(sorted(compared_name.split()))\n",
    "\n",
    "    address_tokens = compared_address.split()\n",
    "    name_tokens = compared_name.split()\n",
    "\n",
    "    # **Lewati jika address_tokens atau name_tokens kosong**\n",
    "    if not address_tokens or not name_tokens:\n",
    "        continue  # Skip ke iterasi berikutnya\n",
    "\n",
    "    # Find the least frequent tokens\n",
    "    least_frequent_token_address = min(address_tokens, key=lambda token: token_counts_address.get(token, float('inf')))\n",
    "    # print(least_frequent_token_address)\n",
    "    least_frequent_token_name = min(name_tokens, key=lambda token: token_counts_name.get(token, float('inf')))\n",
    "    # print(least_frequent_token_name)\n",
    "    # Initialize group for storing similar names\n",
    "    group = []\n",
    "    name_list = [\n",
    "        name_tuple  # Extract the name from the tuple\n",
    "        for name_tuple in address_dict[least_frequent_token_address]\n",
    "        if name_tuple[0] in name_dict.get(least_frequent_token_name, [])\n",
    "    ]\n",
    "\n",
    "    # Compare names using Levenshtein similarity\n",
    "    for name_tuple in name_list:\n",
    "        name = name_tuple[0]\n",
    "        name = \" \".join(sorted(name.split()))\n",
    "        similarity = distance.get_jaro_distance(compared_name, name)\n",
    "        threshold = 0.75 * min(len(compared_name),len(name)) / max(len(compared_name), len(name))\n",
    "        if similarity > threshold:\n",
    "            group.append(name_tuple)\n",
    "            similarity_list[(compared_name,compared_address)].append(similarity)\n",
    "\n",
    "    # Append to final_blocks instead of overwriting\n",
    "    final_blocks[(compared_name,compared_address)].extend(group)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name & Tempat Lahir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pob_dict = defaultdict(list)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for idx, row in dataset.iterrows():\n",
    "    # Get the cleaned_name and deep_clean_address\n",
    "    name = row['cleaned_name']\n",
    "    pob = row['TEMPAT_LAHIR']\n",
    "    \n",
    "    # Skip rows with invalid names or places of birth\n",
    "    if not isinstance(name, str) or not name.strip() or not isinstance(pob, str) or not pob.strip():\n",
    "        continue\n",
    "    \n",
    "    # Split the place of birth into tokens\n",
    "    pob_tokens = pob.split()\n",
    "    \n",
    "    # Store tuples (name, place of birth) in pob_dict\n",
    "    for token in pob_tokens:\n",
    "        if (name, pob) not in pob_dict[token]:\n",
    "            pob_dict[token].append((name, pob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = defaultdict(list)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for idx, row in dataset.iterrows():\n",
    "    # Get the cleaned_name\n",
    "    name = row['cleaned_name']\n",
    "    if not isinstance(name, str) or not name.strip():\n",
    "        continue  # Skip rows with empty or invalid names\n",
    "    \n",
    "    # Split the name into tokens\n",
    "    tokens = name.split() \n",
    "    for token in tokens:\n",
    "        if name not in name_dict[token]:  # Avoid duplicates\n",
    "            name_dict[token].append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaro Winkler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 14484.35 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "# Tokenizing pob (place of birth)\n",
    "tokens_pob = [str(pob).split() for pob in dataset['TEMPAT_LAHIR']]\n",
    "all_tokens_pob = [token for sublist in tokens_pob for token in sublist]\n",
    "token_counts_pob = Counter(all_tokens_pob)\n",
    "\n",
    "# Tokenizing names\n",
    "tokens_name = [name.split() for name in dataset['cleaned_name']]\n",
    "all_tokens_name = [token for sublist in tokens_name for token in sublist]\n",
    "token_counts_name = Counter(all_tokens_name)\n",
    "\n",
    "# Initialize final structures\n",
    "final_blocks = defaultdict(list)\n",
    "similarity_list = defaultdict(list)\n",
    "\n",
    "# Iterate through the rows of the dataset\n",
    "start_time = time.time()\n",
    "for index, row in sampled_data_100000.iterrows():\n",
    "    compared_pob = row['TEMPAT_LAHIR']\n",
    "    compared_name = row['cleaned_name']\n",
    "    compared_name = \" \".join(sorted(compared_name.split()))\n",
    "\n",
    "    if pd.isna(compared_pob):\n",
    "        continue\n",
    "    \n",
    "    pob_tokens = compared_pob.split()\n",
    "    name_tokens = compared_name.split()\n",
    "\n",
    "    # **Lewati jika pob_tokens atau name_tokens kosong**\n",
    "    if not pob_tokens or not name_tokens:\n",
    "        continue  # Skip ke iterasi berikutnya\n",
    "\n",
    "    # Find the least frequent tokens\n",
    "    least_frequent_token_pob = min(pob_tokens, key=lambda token: token_counts_pob.get(token, float('inf')))\n",
    "    least_frequent_token_name = min(name_tokens, key=lambda token: token_counts_name.get(token, float('inf')))\n",
    "\n",
    "    # Initialize group for storing similar names\n",
    "    group = []\n",
    "    name_list = [\n",
    "        name_tuple  # Extract the name from the tuple\n",
    "        for name_tuple in pob_dict[least_frequent_token_pob]\n",
    "        if name_tuple[0] in name_dict.get(least_frequent_token_name, [])\n",
    "    ]\n",
    "\n",
    "    # Compare names using Levenshtein similarity\n",
    "    for name_tuple in name_list:\n",
    "        name = name_tuple[0]\n",
    "        name = \" \".join(sorted(name.split()))\n",
    "        similarity = distance.get_jaro_distance(compared_name, name)\n",
    "        threshold = 0.75 * min(len(compared_name),len(name)) / max(len(compared_name), len(name))\n",
    "        if similarity > threshold:\n",
    "            group.append(name_tuple)\n",
    "            similarity_list[(compared_name, compared_pob)].append(similarity)\n",
    "\n",
    "    # Append to final_blocks instead of overwriting\n",
    "    final_blocks[(compared_name, compared_pob)].extend(group)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair Completeness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_monre_0924 = pd.DataFrame(columns=['NO_AGGR', 'cleaned_name', 'SID'])\n",
    "mst_sid_0924 = pd.DataFrame(columns=['SID', 'cleaned_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  NAME ONLY\n",
    "monre_dict = defaultdict(list)\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    monre_dict[row['cleaned_name']].append(index)\n",
    "    \n",
    "# NAME ADDRESS\n",
    "monre_dict = defaultdict(list)\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    monre_dict[(row['cleaned_name'], row['deep_clean_address'])].append(index)\n",
    "\n",
    "# NAME TEMPAT LAHIR\n",
    "monre_dict = defaultdict(list)\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    monre_dict[(row['cleaned_name'], row['TEMPAT_LAHIR'])].append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaro Winkler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [name.split() for name in dataset['cleaned_name']]\n",
    "all_tokens = [token for sublist in tokens for token in sublist]\n",
    "token_counts = Counter(all_tokens)\n",
    "\n",
    "def add_new_name_to_results_dict(new_name):\n",
    "    name_tokens = new_name.split()\n",
    "    least_frequent_token = min(name_tokens, key=lambda token: token_counts[token])\n",
    "    group = []\n",
    "    # new_name = \" \".join(sorted(new_name.split()))\n",
    "\n",
    "    for name in tokens_dict[least_frequent_token]:\n",
    "        # name_sort = \" \".join(sorted(name.split()))\n",
    "        similarity = distance.get_jaro_distance(new_name, name)\n",
    "        if similarity >= 0.75:\n",
    "            group.append(name)\n",
    "    return group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name & Address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaro Winkler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['deep_clean_address'] = dataset['deep_clean_address'].fillna(dataset['ALMT_RUMAH']).astype(str)\n",
    "sampled_data_100000['deep_clean_address'] = sampled_data_100000['deep_clean_address'].fillna(sampled_data_100000['ALMT_RUMAH']).astype(str)\n",
    "\n",
    "# Tokenizing addresses\n",
    "tokens_address = [str(address).split() for address in dataset['deep_clean_address']]\n",
    "all_tokens_address = [token for sublist in tokens_address for token in sublist]\n",
    "token_counts_address = Counter(all_tokens_address)\n",
    "\n",
    "# Tokenizing names\n",
    "tokens_name = [name.split() for name in dataset['cleaned_name']]\n",
    "all_tokens_name = [token for sublist in tokens_name for token in sublist]\n",
    "token_counts_name = Counter(all_tokens_name)\n",
    "\n",
    "def add_new_name_to_results_dict(new_name, new_address):\n",
    "    address_tokens = new_address.split()\n",
    "    name_tokens = new_name.split()\n",
    "    if address_tokens:\n",
    "        # Find the least frequent tokens\n",
    "        least_frequent_token_address = min(address_tokens, key=lambda token: token_counts_address.get(token, float('inf')))\n",
    "        least_frequent_token_name = min(name_tokens, key=lambda token: token_counts_name.get(token, float('inf')))\n",
    "\n",
    "        # Initialize group for storing similar names\n",
    "        group = []\n",
    "\n",
    "        name_list = [\n",
    "            name_tuple  # Extract the name from the tuple\n",
    "            for name_tuple in address_dict[least_frequent_token_address]\n",
    "            if name_tuple[0] in name_dict.get(least_frequent_token_name, [])\n",
    "        ]\n",
    "\n",
    "        # Compare names using Levenshtein similarity\n",
    "        for name_tuple in name_list:\n",
    "            name = name_tuple[0]\n",
    "            similarity = distance.get_jaro_distance(new_name, name)\n",
    "            if similarity > 0.75 :\n",
    "                group.append(name_tuple)\n",
    "        return group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name & Tempat Lahir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaro Winkler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing pob (place of birth)\n",
    "tokens_pob = [str(pob).split() for pob in dataset['TEMPAT_LAHIR']]\n",
    "all_tokens_pob = [token for sublist in tokens_pob for token in sublist]\n",
    "token_counts_pob = Counter(all_tokens_pob)\n",
    "\n",
    "# Tokenizing names\n",
    "tokens_name = [name.split() for name in dataset['cleaned_name']]\n",
    "all_tokens_name = [token for sublist in tokens_name for token in sublist]\n",
    "token_counts_name = Counter(all_tokens_name)\n",
    "\n",
    "# Initialize final structures\n",
    "final_blocks = defaultdict(list)\n",
    "similarity_list = defaultdict(list)\n",
    "\n",
    "# Iterate through the rows of the dataset\n",
    "def add_new_name_to_results_dict(compared_name, compared_pob):\n",
    "    pob_tokens = compared_pob.split()\n",
    "    name_tokens = compared_name.split()\n",
    "\n",
    "    # Find the least frequent tokens\n",
    "    least_frequent_token_pob = min(pob_tokens, key=lambda token: token_counts_pob.get(token, float('inf')))\n",
    "    least_frequent_token_name = min(name_tokens, key=lambda token: token_counts_name.get(token, float('inf')))\n",
    "\n",
    "    # Initialize group for storing similar names\n",
    "    group = []\n",
    "    name_list = [\n",
    "        name_tuple  # Extract the name from the tuple\n",
    "        for name_tuple in pob_dict[least_frequent_token_pob]\n",
    "        if name_tuple[0] in name_dict.get(least_frequent_token_name, [])\n",
    "    ]\n",
    "\n",
    "    # Compare names using Levenshtein similarity\n",
    "    for name_tuple in name_list:\n",
    "        name = name_tuple[0]\n",
    "        similarity = distance.get_jaro_distance(compared_name, name)\n",
    "        if similarity > 0.75:\n",
    "            group.append(name_tuple)\n",
    "    return group\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaro_winkler_match(value1, value2, threshold=0.92):\n",
    "    if pd.notna(value1) and pd.notna(value2):\n",
    "        value1_str = str(value1).strip() \n",
    "        value2_str = str(value2).strip() \n",
    "        \n",
    "        # Ensure neither value is an empty string\n",
    "        if value1_str and value2_str:\n",
    "            similarity = distance.get_jaro_distance(value1_str, value2_str)\n",
    "            return similarity >= threshold\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_5156\\2312160529.py:118: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  filtered_result_df = pd.concat([filtered_result_df, row_compared_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 28378.81 seconds\n"
     ]
    }
   ],
   "source": [
    "# NAME ONLY\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "no_aggr_set = set(sid_monre_0924['NO_AGGR'])\n",
    "\n",
    "for index, row in sampled_data_100000.iterrows():\n",
    "    if row['NO_AGGR'] not in no_aggr_set:\n",
    "        row_compared = row\n",
    "        matched_dfs = [] \n",
    "        results_list = add_new_name_to_results_dict(row['cleaned_name'])\n",
    "        for name in results_list:\n",
    "            if name in monre_dict:\n",
    "                indices = monre_dict[name]\n",
    "                matched_dfs.append(dataset.loc[indices])  \n",
    "\n",
    "        if matched_dfs:\n",
    "            matched_df = pd.concat(matched_dfs, ignore_index=True)\n",
    "        else:\n",
    "            matched_df = pd.DataFrame()\n",
    "\n",
    "        if not matched_df.empty:\n",
    "            result_df_nodup = matched_df.drop_duplicates(subset='NO_AGGR').reset_index(drop=True)\n",
    "        else:\n",
    "            print(row['cleaned_name'])\n",
    "            result_df_nodup = pd.DataFrame()\n",
    "\n",
    "        time_start = time.time()\n",
    "        aggr_compared = row['NO_AGGR']\n",
    "        name_compared = row['cleaned_name']\n",
    "        dob_compared = row['TGL_LAHIR']\n",
    "        tempat_compared = row['cleaned_TEMPAT_LAHIR']\n",
    "        ktp_kitas_compared = row['cleaned_no_ktp']\n",
    "        mother_name_compared = row['cleaned_NAMA_IBU_KANDUNG'] \n",
    "        npwp_compared = row['cleaned_no_npwp']\n",
    "        address_compared = row['cleaned_alamat']\n",
    "\n",
    "        # Filter result_df based on matching criteria\n",
    "        filtered_result_df = result_df_nodup[\n",
    "            ((pd.notna(result_df_nodup['TGL_LAHIR']) & pd.notna(dob_compared) & \n",
    "            (result_df_nodup['TGL_LAHIR'] == dob_compared)) |\n",
    "            (pd.notna(result_df_nodup['cleaned_no_ktp']) & pd.notna(ktp_kitas_compared) & \n",
    "            (result_df_nodup['cleaned_no_ktp'] == ktp_kitas_compared)) |\n",
    "            (pd.notna(result_df_nodup['cleaned_NAMA_IBU_KANDUNG']) & pd.notna(mother_name_compared) & \n",
    "            (result_df_nodup['cleaned_NAMA_IBU_KANDUNG'] == mother_name_compared)) |\n",
    "            (pd.notna(result_df_nodup['cleaned_no_npwp']) & pd.notna(npwp_compared) & \n",
    "            (result_df_nodup['cleaned_no_npwp'] == npwp_compared))\n",
    "        )]\n",
    "        \n",
    "        filtered_result_df = filtered_result_df.copy()\n",
    "        filtered_result_df['flag_SID'] = 'N'  \n",
    "        filtered_result_df['rule_num'] = None\n",
    "\n",
    "        for index, row in filtered_result_df.iterrows():\n",
    "            name_sim = jaro_winkler_match(row['cleaned_name'], name_compared)\n",
    "            \n",
    "            if (\n",
    "                name_sim and \n",
    "                pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared and \n",
    "                pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  # Rule 1\n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 1'\n",
    "                # print('MATCHING RULE 1 APPLIED')\n",
    "\n",
    "            elif (\n",
    "                name_sim and \n",
    "                pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 2\n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 2'\n",
    "                # print('MATCHING RULE 2 APPLIED')\n",
    "\n",
    "            elif (\n",
    "                name_sim and \n",
    "                pd.notna(row['cleaned_no_npwp']) and row['cleaned_no_npwp'] == npwp_compared  # Rule 3\n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 3'\n",
    "                # print('MATCHING RULE 3 APPLIED')\n",
    "\n",
    "            elif (\n",
    "                jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and  # Rule 4\n",
    "                pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  \n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 4' \n",
    "                # print('MATCHING RULE 4 APPLIED')\n",
    "\n",
    "            elif (\n",
    "                pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 5\n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 5'  \n",
    "                # print('MATCHING RULE 5 APPLIED')\n",
    "            \n",
    "            elif (\n",
    "                jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and\n",
    "                pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared # Rule 6\n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 6' \n",
    "                # print('MATCHING RULE 6 APPLIED')\n",
    "\n",
    "            else:\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'N'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = None \n",
    "\n",
    "        filtered_result_df = filtered_result_df[filtered_result_df['flag_SID'] == 'Y'].reset_index(drop=True)\n",
    "        \n",
    "        if filtered_result_df.empty:\n",
    "            row_compared_df = pd.DataFrame([row_compared])  \n",
    "            row_compared_df['flag_SID'] = 'Y'              \n",
    "            filtered_result_df = pd.concat([filtered_result_df, row_compared_df], ignore_index=True)\n",
    "\n",
    "        check_sid_exist = False\n",
    "        for no_aggr in filtered_result_df['NO_AGGR']:\n",
    "            if no_aggr in no_aggr_set:\n",
    "                check_sid_exist = True\n",
    "                filtered_result_df['SID'] = sid_monre_0924.loc[sid_monre_0924['NO_AGGR'] == no_aggr, 'SID'].values[0]\n",
    "                break\n",
    "            \n",
    "        if check_sid_exist:\n",
    "            count += 1\n",
    "\n",
    "        if not check_sid_exist:\n",
    "            filtered_result_df = filtered_result_df.sort_values(by='DT_GOLIVE_VALID').reset_index(drop=True)\n",
    "            \n",
    "            kode_cabang = str(filtered_result_df.loc[0, 'CD_SP'])\n",
    "            \n",
    "            matching_sid_rows = mst_sid_0924[mst_sid_0924['SID'].str[:6] == kode_cabang]\n",
    "            last_sequence = matching_sid_rows['SID'].str[-7:].astype(int).max() if not matching_sid_rows.empty else 0\n",
    "            \n",
    "            filtered_result_df['SID'] = f\"{kode_cabang}{(last_sequence + 1):07d}\"\n",
    "\n",
    "        rows_to_append_filtered = filtered_result_df[~filtered_result_df['NO_AGGR'].isin(no_aggr_set)]\n",
    "\n",
    "        if not rows_to_append_filtered.empty:  \n",
    "            sid_monre_0924 = pd.concat([sid_monre_0924, rows_to_append_filtered[['NO_AGGR', 'cleaned_name', 'SID', 'rule_num']]], ignore_index=True)\n",
    "            no_aggr_set.update(rows_to_append_filtered['NO_AGGR'].tolist())\n",
    "            if not check_sid_exist:\n",
    "                rows_to_append_2 = rows_to_append_filtered.iloc[[0]][['cleaned_name', 'SID']]\n",
    "                mst_sid_0924 = pd.concat([mst_sid_0924, rows_to_append_2], ignore_index=True)\n",
    "\n",
    "mst_sid_0924.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_5156\\2733074365.py:115: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  filtered_result_df = pd.concat([filtered_result_df, row_compared_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 13348.83 seconds\n"
     ]
    }
   ],
   "source": [
    "# NAME & ADDRESS\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "no_aggr_set = set(sid_monre_0924['NO_AGGR'])\n",
    "\n",
    "for index, row in sampled_data_100000.iterrows():\n",
    "    if row['NO_AGGR'] not in no_aggr_set:\n",
    "        row_compared = row\n",
    "        matched_dfs = []\n",
    "        if row['deep_clean_address'] and row['deep_clean_address'] != 'nan':\n",
    "            results_list = add_new_name_to_results_dict(row['cleaned_name'], row['deep_clean_address'])\n",
    "            for tuple in results_list:\n",
    "                if tuple in monre_dict:\n",
    "                    indices = monre_dict[tuple]\n",
    "                    matched_dfs.append(dataset.loc[indices])  \n",
    "\n",
    "            if matched_dfs:\n",
    "                matched_df = pd.concat(matched_dfs, ignore_index=True)\n",
    "                result_df_nodup = matched_df.drop_duplicates(subset='NO_AGGR').reset_index(drop=True)\n",
    "\n",
    "                aggr_compared = row['NO_AGGR']\n",
    "                name_compared = row['cleaned_name']\n",
    "                dob_compared = row['TGL_LAHIR']\n",
    "                tempat_compared = row['cleaned_TEMPAT_LAHIR']\n",
    "                ktp_kitas_compared = row['cleaned_no_ktp']\n",
    "                mother_name_compared = row['cleaned_NAMA_IBU_KANDUNG'] \n",
    "                npwp_compared = row['cleaned_no_npwp']\n",
    "                address_compared = row['cleaned_alamat']\n",
    "\n",
    "                # Filter result_df based on matching criteria\n",
    "                filtered_result_df = result_df_nodup[\n",
    "                    ((pd.notna(result_df_nodup['TGL_LAHIR']) & pd.notna(dob_compared) & \n",
    "                    (result_df_nodup['TGL_LAHIR'] == dob_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_no_ktp']) & pd.notna(ktp_kitas_compared) & \n",
    "                    (result_df_nodup['cleaned_no_ktp'] == ktp_kitas_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_NAMA_IBU_KANDUNG']) & pd.notna(mother_name_compared) & \n",
    "                    (result_df_nodup['cleaned_NAMA_IBU_KANDUNG'] == mother_name_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_no_npwp']) & pd.notna(npwp_compared) & \n",
    "                    (result_df_nodup['cleaned_no_npwp'] == npwp_compared))\n",
    "                )]\n",
    "                \n",
    "                filtered_result_df = filtered_result_df.copy()\n",
    "                filtered_result_df['flag_SID'] = 'N'  \n",
    "                filtered_result_df['rule_num'] = None\n",
    "\n",
    "                for index, row in filtered_result_df.iterrows():\n",
    "                    name_sim = jaro_winkler_match(row['cleaned_name'], name_compared)\n",
    "                    \n",
    "                    if (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared and \n",
    "                        pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  # Rule 1\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 1'\n",
    "                        # print('MATCHING RULE 1 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 2\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 2'\n",
    "                        # print('MATCHING RULE 2 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['cleaned_no_npwp']) and row['cleaned_no_npwp'] == npwp_compared  # Rule 3\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 3'\n",
    "                        # print('MATCHING RULE 3 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                        pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and  # Rule 4\n",
    "                        pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  \n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 4' \n",
    "                        # print('MATCHING RULE 4 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 5\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 5'  \n",
    "                        # print('MATCHING RULE 5 APPLIED')\n",
    "                    \n",
    "                    elif (\n",
    "                        jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                        pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and\n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared # Rule 6\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 6' \n",
    "                        # print('MATCHING RULE 6 APPLIED')\n",
    "\n",
    "                    else:\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'N'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = None \n",
    "\n",
    "                filtered_result_df = filtered_result_df[filtered_result_df['flag_SID'] == 'Y'].reset_index(drop=True)\n",
    "                \n",
    "                if filtered_result_df.empty:\n",
    "                    row_compared_df = pd.DataFrame([row_compared])  \n",
    "                    row_compared_df['flag_SID'] = 'Y'              \n",
    "                    filtered_result_df = pd.concat([filtered_result_df, row_compared_df], ignore_index=True)\n",
    "\n",
    "                check_sid_exist = False\n",
    "                for no_aggr in filtered_result_df['NO_AGGR']:\n",
    "                    if no_aggr in no_aggr_set:\n",
    "                        check_sid_exist = True\n",
    "                        filtered_result_df['SID'] = sid_monre_0924.loc[sid_monre_0924['NO_AGGR'] == no_aggr, 'SID'].values[0]\n",
    "                        break\n",
    "                    \n",
    "                if check_sid_exist:\n",
    "                    count += 1\n",
    "\n",
    "                if not check_sid_exist:\n",
    "                    filtered_result_df = filtered_result_df.sort_values(by='DT_GOLIVE_VALID').reset_index(drop=True)\n",
    "                    \n",
    "                    kode_cabang = str(filtered_result_df.loc[0, 'CD_SP'])\n",
    "                    \n",
    "                    matching_sid_rows = mst_sid_0924[mst_sid_0924['SID'].str[:6] == kode_cabang]\n",
    "                    last_sequence = matching_sid_rows['SID'].str[-7:].astype(int).max() if not matching_sid_rows.empty else 0\n",
    "                    \n",
    "                    filtered_result_df['SID'] = f\"{kode_cabang}{(last_sequence + 1):07d}\"\n",
    "\n",
    "                rows_to_append_filtered = filtered_result_df[~filtered_result_df['NO_AGGR'].isin(no_aggr_set)]\n",
    "\n",
    "                if not rows_to_append_filtered.empty:  \n",
    "                    sid_monre_0924 = pd.concat([sid_monre_0924, rows_to_append_filtered[['NO_AGGR', 'cleaned_name', 'SID', 'rule_num']]], ignore_index=True)\n",
    "                    no_aggr_set.update(rows_to_append_filtered['NO_AGGR'].tolist())\n",
    "                    if not check_sid_exist:\n",
    "                        rows_to_append_2 = rows_to_append_filtered.iloc[[0]][['cleaned_name', 'SID']]\n",
    "                        mst_sid_0924 = pd.concat([mst_sid_0924, rows_to_append_2], ignore_index=True)\n",
    "\n",
    "mst_sid_0924.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 28200.41 seconds\n"
     ]
    }
   ],
   "source": [
    "# NAME & TEMPAT LAHIR\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "no_aggr_set = set(sid_monre_0924['NO_AGGR'])\n",
    "\n",
    "for index, row in sampled_data_100000.iterrows():\n",
    "    if row['NO_AGGR'] not in no_aggr_set:\n",
    "        row_compared = row\n",
    "        matched_dfs = []\n",
    "        if pd.notna(row_compared['TEMPAT_LAHIR']) and row_compared['TEMPAT_LAHIR'] != 'nan':\n",
    "            results_list = add_new_name_to_results_dict(row['cleaned_name'], row['TEMPAT_LAHIR'])\n",
    "            for tuple in results_list:\n",
    "                if tuple in monre_dict:\n",
    "                    indices = monre_dict[tuple]\n",
    "                    matched_dfs.append(dataset.loc[indices])  \n",
    "\n",
    "            if matched_dfs:\n",
    "                matched_df = pd.concat(matched_dfs, ignore_index=True)\n",
    "                result_df_nodup = matched_df.drop_duplicates(subset='NO_AGGR').reset_index(drop=True)\n",
    "            # else:\n",
    "            #     result_df_nodup = pd.DataFrame()\n",
    "            #     print(row['cleaned_name'])\n",
    "            #     print(row['deep_clean_address'])\n",
    "\n",
    "                aggr_compared = row['NO_AGGR']\n",
    "                name_compared = row['cleaned_name']\n",
    "                dob_compared = row['TGL_LAHIR']\n",
    "                tempat_compared = row['cleaned_TEMPAT_LAHIR']\n",
    "                ktp_kitas_compared = row['cleaned_no_ktp']\n",
    "                mother_name_compared = row['cleaned_NAMA_IBU_KANDUNG'] \n",
    "                npwp_compared = row['cleaned_no_npwp']\n",
    "                address_compared = row['cleaned_alamat']\n",
    "\n",
    "                # Filter result_df based on matching criteria\n",
    "                filtered_result_df = result_df_nodup[\n",
    "                    ((pd.notna(result_df_nodup['TGL_LAHIR']) & pd.notna(dob_compared) & \n",
    "                    (result_df_nodup['TGL_LAHIR'] == dob_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_no_ktp']) & pd.notna(ktp_kitas_compared) & \n",
    "                    (result_df_nodup['cleaned_no_ktp'] == ktp_kitas_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_NAMA_IBU_KANDUNG']) & pd.notna(mother_name_compared) & \n",
    "                    (result_df_nodup['cleaned_NAMA_IBU_KANDUNG'] == mother_name_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_no_npwp']) & pd.notna(npwp_compared) & \n",
    "                    (result_df_nodup['cleaned_no_npwp'] == npwp_compared))\n",
    "                )]\n",
    "                \n",
    "                filtered_result_df = filtered_result_df.copy()\n",
    "                filtered_result_df['flag_SID'] = 'N'  \n",
    "                filtered_result_df['rule_num'] = None\n",
    "\n",
    "                for index, row in filtered_result_df.iterrows():\n",
    "                    name_sim = jaro_winkler_match(row['cleaned_name'], name_compared)\n",
    "                    \n",
    "                    if (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared and \n",
    "                        pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  # Rule 1\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 1'\n",
    "                        # print('MATCHING RULE 1 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 2\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 2'\n",
    "                        # print('MATCHING RULE 2 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['cleaned_no_npwp']) and row['cleaned_no_npwp'] == npwp_compared  # Rule 3\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 3'\n",
    "                        # print('MATCHING RULE 3 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                        pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and  # Rule 4\n",
    "                        pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  \n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 4' \n",
    "                        # print('MATCHING RULE 4 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 5\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 5'  \n",
    "                        # print('MATCHING RULE 5 APPLIED')\n",
    "                    \n",
    "                    elif (\n",
    "                        jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                        pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and\n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared # Rule 6\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 6' \n",
    "                        # print('MATCHING RULE 6 APPLIED')\n",
    "\n",
    "                    else:\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'N'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = None \n",
    "\n",
    "                filtered_result_df = filtered_result_df[filtered_result_df['flag_SID'] == 'Y'].reset_index(drop=True)\n",
    "                \n",
    "                if filtered_result_df.empty:\n",
    "                    row_compared_df = pd.DataFrame([row_compared])  \n",
    "                    row_compared_df['flag_SID'] = 'Y'              \n",
    "                    filtered_result_df = pd.concat([filtered_result_df, row_compared_df], ignore_index=True)\n",
    "\n",
    "                check_sid_exist = False\n",
    "                for no_aggr in filtered_result_df['NO_AGGR']:\n",
    "                    if no_aggr in no_aggr_set:\n",
    "                        check_sid_exist = True\n",
    "                        filtered_result_df['SID'] = sid_monre_0924.loc[sid_monre_0924['NO_AGGR'] == no_aggr, 'SID'].values[0]\n",
    "                        break\n",
    "                    \n",
    "                if check_sid_exist:\n",
    "                    count += 1\n",
    "\n",
    "                if not check_sid_exist:\n",
    "                    filtered_result_df = filtered_result_df.sort_values(by='DT_GOLIVE_VALID').reset_index(drop=True)\n",
    "                    \n",
    "                    kode_cabang = str(filtered_result_df.loc[0, 'CD_SP'])\n",
    "                    \n",
    "                    matching_sid_rows = mst_sid_0924[mst_sid_0924['SID'].str[:6] == kode_cabang]\n",
    "                    last_sequence = matching_sid_rows['SID'].str[-7:].astype(int).max() if not matching_sid_rows.empty else 0\n",
    "                    \n",
    "                    filtered_result_df['SID'] = f\"{kode_cabang}{(last_sequence + 1):07d}\"\n",
    "\n",
    "                rows_to_append_filtered = filtered_result_df[~filtered_result_df['NO_AGGR'].isin(no_aggr_set)]\n",
    "\n",
    "                if not rows_to_append_filtered.empty:  \n",
    "                    sid_monre_0924 = pd.concat([sid_monre_0924, rows_to_append_filtered[['NO_AGGR', 'cleaned_name', 'SID', 'rule_num']]], ignore_index=True)\n",
    "                    no_aggr_set.update(rows_to_append_filtered['NO_AGGR'].tolist())\n",
    "                    if not check_sid_exist:\n",
    "                        rows_to_append_2 = rows_to_append_filtered.iloc[[0]][['cleaned_name', 'SID']]\n",
    "                        mst_sid_0924 = pd.concat([mst_sid_0924, rows_to_append_2], ignore_index=True)\n",
    "\n",
    "mst_sid_0924.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mst_sid_0924.to_excel(r\"D:\\Kuliah\\SEMESTER 8\\Skripsi\\NO ORDER\\NAME POB\\mst_sid_hvtb_jaro_with_order.xlsx\", index=False)\n",
    "sid_monre_0924.to_excel(r\"D:\\Kuliah\\SEMESTER 8\\Skripsi\\NO ORDER\\NAME POB\\sid_monre_hvtb_jaro_with_order.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Distance :  0.9102346550617827\n"
     ]
    }
   ],
   "source": [
    "# # NAME ONLY\n",
    "# avg_distance = []\n",
    "# for compared_name, distances in similarity_list.items():\n",
    "#     avg_distance.append(np.mean(distances))\n",
    "\n",
    "# NAME ADDRESS\n",
    "avg_distance = []\n",
    "for tuple, distances in similarity_list.items():\n",
    "    avg_distance.append(np.mean(distances))\n",
    "\n",
    "# Calculate the overall average\n",
    "overall_avg_distance = np.mean(avg_distance)\n",
    "print('Average Distance : ',overall_avg_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Lenght : 100000 Data\n",
      "Dataset Lenght : 2095454 Data\n",
      "Reduction Ratio: 0.9999955\n",
      "943908\n"
     ]
    }
   ],
   "source": [
    "# Reduction Ratio\n",
    "n = len(dataset)  # Total dataset size\n",
    "m = len(sampled_data_100000)  # Sample size for blocking\n",
    "total_comparisons_before = n * m\n",
    "\n",
    "print(\"Sample Lenght :\", len(sampled_data_100000), \"Data\")\n",
    "print(\"Dataset Lenght :\", len(dataset), \"Data\")\n",
    "\n",
    "# Calculate Total Comparisons After Blocking\n",
    "comparisons_after_blocking = 0\n",
    "for block in final_blocks.values():\n",
    "    comparisons_after_blocking += len(block)\n",
    "\n",
    "# Calculate Reduction Ratio\n",
    "reduction_ratio = 1 - (comparisons_after_blocking / total_comparisons_before)\n",
    "print(f\"Reduction Ratio: {reduction_ratio:.7f}\")\n",
    "print(comparisons_after_blocking)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
