{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import Levenshtein\n",
    "from pyjarowinkler import distance\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_4520\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_4520\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_4520\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_4520\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_4520\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_4520\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_4520\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_4520\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
      "C:\\Users\\karen\\AppData\\Local\\Temp\\ipykernel_4520\\3546598567.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    r'D:\\Kuliah\\SEMESTER 8\\Skripsi\\Dataset\\Sheet1_SID_DATA_COBORR_PASSANGER_1124.csv',\n",
    "    r'D:\\Kuliah\\SEMESTER 8\\Skripsi\\Dataset\\Sheet2_SID_DATA_COBORR_PASSANGER_1124.csv',\n",
    "    r'D:\\Kuliah\\SEMESTER 8\\Skripsi\\Dataset\\Sheet3_SID_DATA_COBORR_PASSANGER_1124.csv'\n",
    "]\n",
    "dtype_params = {\n",
    "    'NO_AGGR': str,\n",
    "    'NO_NPWP': str,\n",
    "    'NO_KTP_KITAS': str,\n",
    "    'NO_KTP_COBORR': str\n",
    "}\n",
    "parse_dates_params = ['DT_GOLIVE_VALID', 'TGL_LAHIR', 'TGL_LAHIR_COBORR']\n",
    "\n",
    "df = pd.concat(\n",
    "    [pd.read_csv(file, dtype=dtype_params, parse_dates=parse_dates_params) for file in file_paths],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df[df['flag_PC'] == 'P']\n",
    "grouped_df = grouped_df.reset_index(drop=True)\n",
    "\n",
    "dataset = grouped_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing & Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANSING\n",
    "def clean_and_validation(df):\n",
    "    # Cleanse Nama Ibu Kandung\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['NAMA_IBU_KANDUNG'].apply(lambda x: str(x).upper() if pd.notna(x) else '')\n",
    "    \n",
    "    # Cleanse Tempat Lahir\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['TEMPAT_LAHIR'].apply(lambda x: str(x).upper() if pd.notna(x) else '')\n",
    "    # Remove 'CONVERTED', 'OTHERS', 'OTHER' and clean up single letter and repeated characters\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['cleaned_TEMPAT_LAHIR'].apply(lambda x: '' if x in ['CONVERTED', 'OTHERS', 'OTHER'] else x)\n",
    "    # Remove spaces in TEMPAT_LAHIR\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['cleaned_TEMPAT_LAHIR'].str.replace(\" \", \"\", regex=False)\n",
    "    # Check for single letter and repeated characters after cleaning spaces\n",
    "    df['cleaned_TEMPAT_LAHIR'] = df['cleaned_TEMPAT_LAHIR'].apply(lambda x: '' if re.match(r'^[A-Za-z]$', x) or re.match(r'^(.)\\1*$', x) else x)\n",
    "    \n",
    "    df['TGL_LAHIR'] = pd.to_datetime(df['TGL_LAHIR'], errors='coerce')\n",
    "\n",
    "    # 1. Remove prefixes\n",
    "    def cleanse_name(name):\n",
    "        if not isinstance(name, str):\n",
    "            return ''\n",
    "        \n",
    "        # List of known prefixes (expand as needed)\n",
    "        prefixes = [\"IR.\", \"IR\", \"DR.\", \"DR\", \"PROF.\", \"PROF\", \"DRS.\", \"DRS\", \"DRA.\", \"DRA\", \"DRG\", \"DRG.\", \"HJ\", \"HJ.\", \"ALM\", \"ALM.\", \"ALMH\", \"ALMH.\"]\n",
    "        \n",
    "        # Regex to match one or more prefixes followed by spaces, symbols, or the end of the string\n",
    "        prefix_pattern = r'^((?:\\b' + '|'.join(map(re.escape, prefixes)) + r'\\.?)\\s*)+[\\s\\W_]+'\n",
    "        \n",
    "        # Remove the prefixes only when followed by a space, symbol, or the end of the string\n",
    "        name = re.sub(prefix_pattern, '', name).strip()\n",
    "\n",
    "        # Remove underscores as symbols\n",
    "        name = name.replace('_', '')\n",
    "\n",
    "        return name\n",
    "\n",
    "    # Initialize cleaned_name column\n",
    "    df['cleaned_name'] = df['NAME_GOLIVE'].apply(cleanse_name)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(cleanse_name)\n",
    "\n",
    "    # 2. Clean Title\n",
    "    def create_pattern(title_list):\n",
    "        patterns = []\n",
    "        for title in title_list:\n",
    "            pattern = r'\\b' + r'\\s*'.join([re.escape(part) for part in title]) + r'\\b'\n",
    "            patterns.append(pattern)\n",
    "        return re.compile('|'.join(patterns), re.IGNORECASE)\n",
    "\n",
    "    # Title lists\n",
    "    title_4 = [\n",
    "        (\"S\", \"I\", \"K\", \"K\"), (\"S\", \"K\", \"P\", \"M\")\n",
    "    ]\n",
    "    title_3 = [\n",
    "        (\"S\", \"I\", \"A\"), (\"S\", \"TR\", \"AK\"), (\"S\", \"E\", \"AS\"), (\"S\", \"A\", \"B\"), \n",
    "        (\"S\", \"PD\", \"B\"), (\"S\", \"TR\", \"BNS\"), (\"S\", \"BIS\", \"DIG\"), (\"S\", \"K\", \"G\"), \n",
    "        (\"S\", \"FIL\", \"H\"), (\"S\", \"H\", \"H\"), (\"S\", \"K\", \"H\"), (\"S\", \"PD\", \"H\"), \n",
    "        (\"S\", \"SOS\", \"H\"), (\"S\", \"TR\", \"HAN\"), (\"S\", \"E\", \"I\"), (\"S\", \"FIL\", \"I\"), \n",
    "        (\"S\", \"H\", \"I\"), (\"S\", \"KOM\", \"I\"), (\"S\", \"PD\", \"I\"), (\"S\", \"SOS\", \"I\"), \n",
    "        (\"S\", \"HUB\", \"INT\"), (\"S\", \"TR\", \"IP\"), (\"S\", \"I\", \"K\"), (\"S\", \"TR\", \"K\"), \n",
    "        (\"S\", \"TR\", \"KEB\"), (\"S\", \"I\", \"KOM\"), (\"S\", \"TR\", \"KOM\"), (\"S\", \"K\", \"L\"), \n",
    "        (\"S\", \"K\", \"M\"), (\"S\", \"A\", \"N\"), (\"S\", \"A\", \"P\"), (\"S\", \"I\", \"P\"), \n",
    "        (\"S\", \"T\", \"P\"), (\"S\", \"ST\", \"PI\"), (\"S\", \"TR\", \"PI\"), (\"S\", \"I\", \"PTK\"), \n",
    "        (\"S\", \"PD\", \"SD\"), (\"S\", \"PD\", \"SI\"), (\"S\", \"TR\", \"SOS\"), (\"S\", \"E\", \"SY\"), \n",
    "        (\"S\", \"TR\", \"T\"), (\"S\", \"SI\", \"TH\"), (\"M\", \"B\", \"A\")\n",
    "    ]\n",
    "    title_2 = [\n",
    "        (\"S\", \"ADM\"), (\"S\", \"AG\"), (\"S\", \"AK\"), (\"S\", \"ANT\"), (\"S\", \"ARS\"),\n",
    "        (\"S\", \"DES\"), (\"S\", \"DS\"), (\"S\", \"E\"), (\"S\", \"FARM\"), (\"S\", \"FIL\"),\n",
    "        (\"S\", \"FT\"), (\"S\", \"GZ\"), (\"S\", \"H\"), (\"S\", \"HAN\"), (\"S\", \"HUM\"),\n",
    "        (\"S\", \"HUT\"), (\"S\", \"IIP\"), (\"S\", \"IK\"), (\"S\", \"IN\"), (\"S\", \"IP\"),\n",
    "        (\"S\", \"KEB\"), (\"S\", \"KED\"), (\"S\", \"KEL\"), (\"S\", \"KEP\"), (\"S\", \"KG\"),\n",
    "        (\"S\", \"KOM\"), (\"S\", \"LI\"), (\"S\", \"M\"), (\"S\", \"MB\"), (\"S\", \"P\"), (\"I\", \"R\"),\n",
    "        (\"S\", \"PAR\"), (\"S\", \"PD\"), (\"S\", \"PI\"), (\"S\", \"PN\"), (\"S\", \"PSI\"),\n",
    "        (\"S\", \"PT\"), (\"S\", \"PTK\"), (\"S\", \"PWK\"), (\"S\", \"S\"), (\"S\", \"SI\"),\n",
    "        (\"S\", \"SN\"), (\"S\", \"SOS\"), (\"S\", \"ST\"), (\"S\", \"STAT\"), (\"S\", \"STP\"),\n",
    "        (\"S\", \"SY\"), (\"S\", \"T\"), (\"S\", \"TH\"), (\"S\", \"TI\"), (\"M\", \"T\"), (\"S\", \"E\"), (\"PH\", \"D\"), (\"S\", \"AKTR\"),  (\"M\", \"E\")\n",
    "    ]\n",
    "\n",
    "    # Compile all patterns\n",
    "    pattern_4char = create_pattern(title_4)\n",
    "    pattern_3char = create_pattern(title_3)\n",
    "    pattern_2char = create_pattern(title_2)\n",
    "\n",
    "    def clean_name(name):\n",
    "        if not isinstance(name, str):  # Ensure input is a string\n",
    "            return ''\n",
    "\n",
    "        # Remove non-word characters and symbols\n",
    "        name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    \n",
    "\n",
    "        # Remove numeric characters\n",
    "        name = ''.join([char for char in name if not char.isdigit()])\n",
    "\n",
    "        # Iteratively remove titles until no match is found\n",
    "        while True:\n",
    "            old_name = name\n",
    "            name = pattern_4char.sub('', name)\n",
    "            name = pattern_3char.sub('', name)\n",
    "            name = pattern_2char.sub('', name)\n",
    "            if name == old_name:  # Stop if no further changes\n",
    "                break\n",
    "\n",
    "        return name.strip()\n",
    "\n",
    "    # Apply name cleaning\n",
    "    df['cleaned_name'] = df['cleaned_name'].apply(clean_name)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(clean_name)\n",
    "    \n",
    "    # remove first letter in name\n",
    "    #df['cleaned_name'] = df['cleaned_name'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()[0]) == 1 else x)\n",
    "    #df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()[0]) == 1 else x)\n",
    "    df['cleaned_name'] = df['cleaned_name'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()) > 1 and len(x.split()[0]) == 1 else x)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(lambda x: ' '.join(x.split()[1:]) if len(x.split()) > 1 and len(x.split()[0]) == 1 else x)\n",
    "\n",
    "    # 3. Clean NO_KTP_KITAS column\n",
    "    df['cleaned_no_ktp'] = df['NO_KTP_KITAS'].astype(str).str.replace(r'\\D', '', regex=True)\n",
    "\n",
    "    # Function to clean the KTP values\n",
    "    def clean_no_ktp(ktp):\n",
    "        # If length is 1 or the value consists of repeating characters (e.g., \"1111111111111111\", \"XXXX\", etc.)\n",
    "        if len(ktp) == 1 or len(set(ktp)) == 1:  \n",
    "            return None  # Return None for invalid cases\n",
    "        return ktp\n",
    "\n",
    "    # Apply the cleaning function to the 'NO_KTP_KITAS' column\n",
    "    df['cleaned_no_ktp'] = df['cleaned_no_ktp'].apply(clean_no_ktp)\n",
    "\n",
    "    # 4. Clean NO_NPWP column\n",
    "    df['cleaned_no_npwp'] = df['NO_NPWP'].apply(lambda x: ''.join(filter(str.isdigit, str(x))))\n",
    "\n",
    "\n",
    "    # 5. Clean Address\n",
    "    substitutions = [\n",
    "        (r'\\b(JLN\\s|JLN\\.|JALAN\\s|JALAN\\.|JL\\.|JLH\\s|JLH\\.)\\s?', 'JL '),\n",
    "        (r'\\b(GANG\\s|GANG\\.|GG\\.)\\s?', 'GG '),\n",
    "        (r'\\b(PONDOK\\s|PONDOK\\.|PD\\.)\\s?', 'PD '),\n",
    "        (r'\\b(PERUMAHAN\\s|PERUMAHAN\\.|PERUM\\s|PERUM\\.|PRM\\.|PERUMNAS\\s|PERUMNAS\\.)\\s?', 'PRM '),\n",
    "        (r'\\b(DESA\\s|DESA\\.|DUSUN\\.|DUSUN\\s|DS\\.|DSN\\.|DSN\\s)\\s?', 'DS '),\n",
    "        (r'\\b(KP\\.|KPG\\.|KPG\\s|KAMPUNG\\s|KAMPUNG\\.|KAMP\\s|KAMP\\.|KMP\\s|KMP\\.)\\s?', 'KP '),\n",
    "        (r'\\b(APARTEMEN\\.|APARTEMEN\\s|APART\\.|APART\\s|APARTMENT\\.|APARTMENT\\s|APARTEMENT\\.|APARTEMENT\\s|AP\\s|AP\\.|APT\\.)\\s?', 'APT '),\n",
    "        (r'\\b(KOMPLEKS\\s|KOMPLEKS\\.|KOMPLEK\\s|KOMPLEK\\.|KOMPL\\s|KOMPL\\.|KOMP\\s|KOMP\\.|KOM\\.)\\s?', 'KOM '),\n",
    "        (r'\\b(LINGKUNGAN\\s|LINGKUNGAN\\.|LINGK\\s|LINGK\\.|LKG\\.)\\s?', 'LKG '),\n",
    "        (r'\\b(TAMAN\\s|TAMAN\\.|TMN\\s|TMN\\.|TM\\.)\\s?', 'TM '),\n",
    "        (r'\\b(BLOK\\.|BLOK\\s|BLK\\.|BLK\\s|BL\\.)\\s?', 'BL '),\n",
    "        (r'\\b(VILLA\\s|VILLA\\.|VILA\\s|VILA\\.|VIL\\.)\\s?', 'VIL '),\n",
    "        (r'\\b(GRIYA\\s|GRIYA\\.|GRY\\.)\\s?', 'GRY '),\n",
    "        (r'\\b(ASRAMA\\s|ASRAMA\\.|ASR\\.)\\s?', 'ASR '),\n",
    "        (r'\\b(TANJUNG\\s|TANJUNG\\.|TJ\\.)\\s?', 'TJ ')\n",
    "    ]\n",
    "\n",
    "    # Known abbreviations with potential spacing issues\n",
    "    long_abbreviations = [\"APARTEMEN\", \"APARTEMENT\", \"APARTMENT\", \"TANJUNG\", \"LINGKUNGAN\", \"ASRAMA\", \n",
    "                        \"KOMPLEKS\", \"PERUMAHAN\", \"KAMPUNG\", \"PERUMNAS\", \"PONDOK\"]\n",
    "\n",
    "    # Function to add space between compound words\n",
    "    def add_space_between_compound_words(address):\n",
    "        for abbr in long_abbreviations:\n",
    "            address = re.sub(rf'({abbr})([A-Z])', r'\\1 \\2', address)\n",
    "        return address\n",
    "\n",
    "    # Function to apply all substitutions globally\n",
    "    def update_address(address):\n",
    "        if not isinstance(address, str):\n",
    "            return ''  # Handle non-string inputs gracefully\n",
    "        \n",
    "        # Correct compound abbreviations\n",
    "        address = add_space_between_compound_words(address)\n",
    "        \n",
    "        # Apply all substitution patterns globally\n",
    "        for pattern, replacement in substitutions:\n",
    "            address = re.sub(pattern, replacement, address, flags=re.IGNORECASE)\n",
    "        \n",
    "        return address\n",
    "\n",
    "    df['cleaned_alamat'] = df['ALMT_RUMAH'].apply(update_address)\n",
    "    \n",
    "    abbreviations_to_remove = [\n",
    "        \"JL\", \"GG\", \"PD\", \"PRM\", \"DS\", \"KP\", \"APT\", \"KOM\", \"LKG\", \"TM\", \"BL\", \"VIL\", \"GRY\", \"ASR\", \"TJ\"\n",
    "    ]\n",
    "\n",
    "    # Regex pattern for matching Roman numerals\n",
    "    roman_numeral_pattern = r'\\b(M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3}))\\b'\n",
    "\n",
    "    # Regex pattern to remove \"NO\" or \"NOMOR\" followed by any number and optional spaces\n",
    "    no_nomor_pattern = r'\\b(NO|NOMOR)\\s?\\d*\\b'\n",
    "\n",
    "    # Function to remove abbreviations, numbers, Roman numerals, and \"NO\" or \"NOMOR\" followed by numbers from address\n",
    "    def deep_clean_address(address):\n",
    "        if not isinstance(address, str):\n",
    "            return ''  # Handle non-string inputs gracefully\n",
    "        \n",
    "        # Remove abbreviations\n",
    "        for abbr in abbreviations_to_remove:\n",
    "            address = re.sub(rf'\\b{abbr}\\b\\s?', '', address, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Remove all numbers\n",
    "        address = re.sub(r'\\d+', '', address)\n",
    "        \n",
    "        # Remove Roman numerals using regex\n",
    "        address = re.sub(roman_numeral_pattern, '', address)\n",
    "        \n",
    "        # Remove \"NO\" or \"NOMOR\" followed by numbers\n",
    "        address = re.sub(no_nomor_pattern, '', address)\n",
    "        \n",
    "        address = re.sub(r'[^a-zA-Z0-9\\s]', '', address)\n",
    "        \n",
    "        address = re.sub(r'\\s+', ' ', address).strip()\n",
    "        \n",
    "        return address\n",
    "    df['deep_clean_address'] = df['cleaned_alamat'].apply(deep_clean_address)\n",
    "\n",
    "    # Final fallback if name is empty after cleaning\n",
    "    df['cleaned_name'] = df.apply(lambda row: row['cleaned_name'] if row['cleaned_name'] != '' else row['NAME_GOLIVE'], axis=1)    \n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df.apply(lambda row: row['cleaned_NAMA_IBU_KANDUNG'] if row['cleaned_NAMA_IBU_KANDUNG'] != '' else row['NAMA_IBU_KANDUNG'], axis=1) \n",
    "    \n",
    "    # Remove 'BUNDA', 'UMI', 'IBU', 'BU', 'NY'\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(lambda x: '' if x in ['BUNDA', 'UMI', 'IBU', 'BU', 'NY', 'NYONYA'] else x)\n",
    "\n",
    "    # Remove rows with a single letter or repeated characters for cleaned_NAMA_IBU_KANDUNG\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(\n",
    "    lambda x: '' if isinstance(x, str) and (re.match(r'^[A-Za-z]$', x) or re.match(r'^(.)\\1*$', x)) else x)\n",
    "\n",
    "    def remove_extra_spaces(text):\n",
    "        # Check if the input is a string or not\n",
    "        if not isinstance(text, str):\n",
    "            return ''  # Return an empty string for non-strings (e.g., NaN, float)\n",
    "        return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    df['cleaned_name'] = df['cleaned_name'].apply(remove_extra_spaces)\n",
    "    df['cleaned_NAMA_IBU_KANDUNG'] = df['cleaned_NAMA_IBU_KANDUNG'].apply(remove_extra_spaces)\n",
    "    df['cleaned_alamat'] = df['cleaned_alamat'].apply(remove_extra_spaces)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the clean_and_validation function\n",
    "dataset = clean_and_validation(dataset)\n",
    "backup_dataset = dataset.copy()\n",
    "dataset.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data_100000 = pd.read_excel(r'D:\\Kuliah\\SEMESTER 8\\Skripsi\\Dataset\\sampled_data_100000.xlsx')\n",
    "sampled_data_100 = pd.read_excel(r'D:\\Kuliah\\SEMESTER 8\\Skripsi\\Dataset\\sampled_data_100.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIGRAM Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching bigrams: 100%|██████████| 975949/975949 [00:17<00:00, 55851.30it/s]\n",
      "Processing bigrams: 100%|██████████| 975949/975949 [00:18<00:00, 52328.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 37.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to generate bigrams of 2 characters from a name\n",
    "def get_bigrams(name):\n",
    "    name = name.replace(\" \", \"\")  # Remove spaces\n",
    "    return set(\"\".join(pair) for pair in zip(name, name[1:]))\n",
    "\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Cache bigrams for each unique name\n",
    "unique_cleaned_names = dataset[\"cleaned_name\"].unique()\n",
    "\n",
    "# Precompute bigrams for each name\n",
    "bigrams_cache = {\n",
    "    name: get_bigrams(name)\n",
    "    for name in tqdm(unique_cleaned_names, desc=\"Caching bigrams\")\n",
    "}\n",
    "\n",
    "# Step 2: Generate bigram groups and counts\n",
    "bigram_groups = defaultdict(set)\n",
    "bigram_counts = Counter()\n",
    "\n",
    "# Process bigrams and build groups\n",
    "for name, bigrams in tqdm(bigrams_cache.items(), desc=\"Processing bigrams\"):\n",
    "    for bigram in bigrams:\n",
    "        bigram_counts[bigram] += 1\n",
    "        bigram_groups[bigram].add(name)\n",
    "\n",
    "# Step 3: Filter bigrams by frequency threshold\n",
    "total_names = len(unique_cleaned_names)\n",
    "bigram_frequency_threshold = total_names * 0.25\n",
    "\n",
    "# filtered_bigram_groups = {\n",
    "#     bigram: names\n",
    "#     for bigram, names in bigram_groups.items()\n",
    "#     if bigram_counts[bigram] <= bigram_frequency_threshold\n",
    "# }\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAME ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bigrams_and_jaccard(name1, name2):\n",
    "    name1 = \"\".join(sorted(name1.replace(\" \", \"\")))  # Hilangkan spasi dan urutkan karakter\n",
    "    name2 = \"\".join(sorted(name2.replace(\" \", \"\")))  \n",
    "\n",
    "    set1, set2 = set(name1), set(name2)  # Buat himpunan karakter unik\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "\n",
    "    similarity = intersection / union if union > 0 else 0  # Jaccard similarity\n",
    "    return similarity\n",
    "\n",
    "# Function to calculate Jaro-Winkler distances for names based on selected bigrams\n",
    "def calculate_jaccard_distances(name, similarity_list):\n",
    "    bigrams = bigrams_cache[name]  # Use precomputed bigrams\n",
    "    bigram_weights = [\n",
    "        (bigram, bigram_counts[bigram]) for bigram in bigrams if bigram in bigram_groups\n",
    "        and bigram_counts[bigram] <= bigram_frequency_threshold  # Filter bigrams based on frequency\n",
    "    ]\n",
    "\n",
    "    # Sort bigrams by their frequency (lower frequency = higher weight)\n",
    "    bigram_weights_sorted = sorted(bigram_weights, key=lambda x: x[1])\n",
    "\n",
    "    # Select the top 3 least frequent bigrams\n",
    "    selected_bigrams = [bg[0] for bg in bigram_weights_sorted[:3]]\n",
    "\n",
    "    # Find all unique names in the groups of these 3 bigrams\n",
    "    matching_names = set()\n",
    "    for bigram in selected_bigrams:\n",
    "        matching_names.update(bigram_groups[bigram])\n",
    "\n",
    "    matching_names.discard(name)  # Remove the original name from comparison\n",
    "    \n",
    "    # Compute Jaro-Winkler similarity for each name in the matching group using bigram comparison\n",
    "    distances = {}\n",
    "    \n",
    "    for other_name in matching_names:\n",
    "        # Use the new comparison function to check bigram overlap and compute Jaro-Winkler\n",
    "        similarity = compare_bigrams_and_jaccard(name, other_name)\n",
    "        if similarity and similarity > 0.75:\n",
    "            distances[other_name] = similarity\n",
    "            similarity_list[name].append(similarity)\n",
    "    return name, distances  # Return the name and its distances\n",
    "\n",
    "def add_new_name_to_results_dict(new_name, similarity_list):\n",
    "    # Generate bigrams for the new name\n",
    "    new_bigrams = get_bigrams(new_name)\n",
    "    \n",
    "    # Add the new bigrams to the cache\n",
    "    bigrams_cache[new_name] = new_bigrams\n",
    "    \n",
    "    # Calculate Jaro-Winkler distances for the new name\n",
    "    name, distances = calculate_jaccard_distances(new_name, similarity_list)\n",
    "    \n",
    "    # If the new name has similar names, add it to results_list\n",
    "    if distances:\n",
    "        similar_names_df = pd.DataFrame(\n",
    "            list(distances.items()), columns=[\"similar_name\", \"similarity\"]\n",
    "        )\n",
    "        results_list = similar_names_df[\"similar_name\"].tolist()  # Convert similar names to a list\n",
    "    else:\n",
    "        results_list = []\n",
    "    results_list.append(new_name)\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bigrams_and_jaccard(name1, name2):\n",
    "    name1 = \"\".join(sorted(name1.replace(\" \", \"\")))  # Hilangkan spasi dan urutkan karakter\n",
    "    name2 = \"\".join(sorted(name2.replace(\" \", \"\")))  \n",
    "\n",
    "    set1, set2 = set(name1), set(name2)  # Buat himpunan karakter unik\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "\n",
    "    similarity = intersection / union if union > 0 else 0  # Jaccard similarity\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 44075.19 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "final_blocks = defaultdict(list)\n",
    "similarity_list = defaultdict(list)\n",
    "\n",
    "for index, row in sampled_data_100000.iterrows():\n",
    "    final_blocks[row['cleaned_name']] = add_new_name_to_results_dict(row['cleaned_name'], similarity_list)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAME & ADDRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching name bigrams: 100%|██████████| 975949/975949 [00:25<00:00, 38600.44it/s]\n",
      "Caching address bigrams: 100%|██████████| 424771/424771 [00:12<00:00, 34927.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 61.31 seconds\n"
     ]
    }
   ],
   "source": [
    "# Function to generate bigrams\n",
    "def get_bigrams(text):\n",
    "    if not isinstance(text, str):  # Handle NaN or non-string values\n",
    "        return set()\n",
    "    text = text.replace(\" \", \"\")  # Remove spaces\n",
    "    return set(\"\".join(pair) for pair in zip(text, text[1:]))\n",
    "\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Cache bigrams for each unique name and address\n",
    "unique_cleaned_names = dataset[\"cleaned_name\"].unique()\n",
    "unique_addresses = dataset[\"deep_clean_address\"].unique()\n",
    "\n",
    "# Precompute bigrams\n",
    "bigrams_cache_name = {name: get_bigrams(name) for name in tqdm(unique_cleaned_names, desc=\"Caching name bigrams\")}\n",
    "bigrams_cache_address = {address: get_bigrams(address) for address in tqdm(unique_addresses, desc=\"Caching address bigrams\")}\n",
    "\n",
    "# Step 2: Build bigram groups and counts\n",
    "bigram_groups_name = defaultdict(set)\n",
    "bigram_counts_name = Counter()\n",
    "\n",
    "bigram_groups_address = defaultdict(set)\n",
    "bigram_counts_address = Counter()\n",
    "\n",
    "# Process names\n",
    "for name, bigrams in bigrams_cache_name.items():\n",
    "    for bigram in bigrams:\n",
    "        bigram_counts_name[bigram] += 1\n",
    "        bigram_groups_name[bigram].add(name)\n",
    "\n",
    "# Process addresses\n",
    "for address, bigrams in bigrams_cache_address.items():\n",
    "    for bigram in bigrams:\n",
    "        bigram_counts_address[bigram] += 1\n",
    "        bigram_groups_address[bigram].add(address)\n",
    "\n",
    "# Step 3: Filtering frequent bigrams\n",
    "total_names = len(unique_cleaned_names)\n",
    "bigram_frequency_threshold = total_names * 0.25 \n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_dict = defaultdict(list)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for idx, row in dataset.iterrows():\n",
    "    # Get the cleaned_name and deep_clean_address\n",
    "    name = row['cleaned_name']\n",
    "    address = row['deep_clean_address']\n",
    "    \n",
    "    # Skip rows with invalid names or addresses\n",
    "    if not isinstance(name, str) or not name.strip() or not isinstance(address, str) or not address.strip():\n",
    "        continue\n",
    "    \n",
    "    address_dict[address].append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bigrams_and_jaccard(name1, name2):\n",
    "    name1 = \"\".join(sorted(name1.replace(\" \", \"\")))  # Hilangkan spasi dan urutkan karakter\n",
    "    name2 = \"\".join(sorted(name2.replace(\" \", \"\")))  \n",
    "\n",
    "    set1, set2 = set(name1), set(name2)  # Buat himpunan karakter unik\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "\n",
    "    similarity = intersection / union if union > 0 else 0  # Jaccard similarity\n",
    "    return similarity\n",
    "    \n",
    "# Function to find similar names based on address bigrams\n",
    "def calculate_jaccard_distances(name, address, similarity_list):\n",
    "    # Step 1: Get the top 3 least frequent bigrams from name and address\n",
    "    bigrams_name = bigrams_cache_name[name]\n",
    "    bigrams_address = bigrams_cache_address[address]\n",
    "\n",
    "    # Sort bigrams by frequency (lower frequency = more unique)\n",
    "    name_bigrams_sorted = sorted(bigrams_name, key=lambda x: bigram_counts_name[x])[:3]\n",
    "    address_bigrams_sorted = sorted(bigrams_address, key=lambda x: bigram_counts_address[x])[:3]\n",
    "  \n",
    "    list_of_names = set()\n",
    "    for bigram in name_bigrams_sorted:\n",
    "        list_of_names.update(bigram_groups_name[bigram])\n",
    "\n",
    "    # Step 2: Find matching names based on address\n",
    "    matching_addresses = set()\n",
    "    for bigram in address_bigrams_sorted:\n",
    "        matching_addresses.update(bigram_groups_address[bigram])\n",
    "        \n",
    "    matching_name_address_pairs = set()\n",
    "    for addr in matching_addresses:\n",
    "        for matched_name in address_dict[addr]:  # Get names associated with this address\n",
    "            if matched_name in list_of_names:\n",
    "                matching_name_address_pairs.add((matched_name, addr))\n",
    "\n",
    "    # Remove the original (name, address) pair\n",
    "    matching_name_address_pairs.discard((name, address))\n",
    "    # matching_names.intersection_update(list_of_names)\n",
    "    \n",
    "    # Step 3: Compute Jaro-Winkler similarity for final filtered names\n",
    "    distances = {}\n",
    "    for other_name, other_address in matching_name_address_pairs:\n",
    "        similarity = compare_bigrams_and_jaccard(name, other_name)\n",
    "        if similarity and similarity > 0.75:\n",
    "            distances[(other_name,other_address)] = similarity\n",
    "            similarity_list[name].append(similarity)\n",
    "    return name, distances\n",
    "\n",
    "def add_new_name_to_results_dict(new_name, new_address, similarity_list):\n",
    "    # Generate bigrams for the new name\n",
    "    new_bigrams = get_bigrams(new_name)\n",
    "    new_bigrams_address = get_bigrams(new_address)\n",
    "    \n",
    "    # Add the new bigrams to the cache\n",
    "    bigrams_cache_name[new_name] = new_bigrams\n",
    "    bigrams_cache_address[new_address] = new_bigrams_address\n",
    "    \n",
    "    # Calculate Jaro-Winkler distances for the new name\n",
    "    name, distances = calculate_jaccard_distances(new_name, new_address, similarity_list)\n",
    "    \n",
    "    # If the new name has similar names, add it to results_list\n",
    "    if distances:\n",
    "        similar_names_df = pd.DataFrame(\n",
    "            list(distances.items()), columns=[\"similar_name_address\", \"similarity\"]\n",
    "        )\n",
    "        results_list = similar_names_df[\"similar_name_address\"].tolist()  # Convert similar names to a list\n",
    "    else:\n",
    "        results_list = []\n",
    "    results_list.append((new_name,new_address))\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 17117.92 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "final_blocks = defaultdict(list)\n",
    "similarity_list = defaultdict(list)\n",
    "\n",
    "for index, row in sampled_data_100000.iterrows():\n",
    "    final_blocks[row['cleaned_name'], row['deep_clean_address']] = add_new_name_to_results_dict(row['cleaned_name'], row['deep_clean_address'],similarity_list)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nama & Tempat Lahir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching name bigrams: 100%|██████████| 975949/975949 [00:17<00:00, 55943.01it/s] \n",
      "Caching POB bigrams: 100%|██████████| 63593/63593 [00:00<00:00, 97297.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 34.32 seconds\n"
     ]
    }
   ],
   "source": [
    "# Function to generate bigrams\n",
    "def get_bigrams(text):\n",
    "    if not isinstance(text, str):  # Handle NaN or non-string values\n",
    "        return set()\n",
    "    text = text.replace(\" \", \"\")  # Remove spaces\n",
    "    return set(\"\".join(pair) for pair in zip(text, text[1:]))\n",
    "\n",
    "# Start measuring time\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Cache bigrams for each unique name and place of birth (pob)\n",
    "unique_cleaned_names = dataset[\"cleaned_name\"].unique()\n",
    "unique_pob = dataset[\"TEMPAT_LAHIR\"].unique()\n",
    "\n",
    "# Precompute bigrams\n",
    "bigrams_cache_name = {name: get_bigrams(name) for name in tqdm(unique_cleaned_names, desc=\"Caching name bigrams\")}\n",
    "bigrams_cache_pob = {pob: get_bigrams(pob) for pob in tqdm(unique_pob, desc=\"Caching POB bigrams\")}\n",
    "\n",
    "# Step 2: Build bigram groups and counts\n",
    "bigram_groups_name = defaultdict(set)\n",
    "bigram_counts_name = Counter()\n",
    "\n",
    "bigram_groups_pob = defaultdict(set)\n",
    "bigram_counts_pob = Counter()\n",
    "\n",
    "# Process names\n",
    "for name, bigrams in bigrams_cache_name.items():\n",
    "    for bigram in bigrams:\n",
    "        bigram_counts_name[bigram] += 1\n",
    "        bigram_groups_name[bigram].add(name)\n",
    "\n",
    "# Process place of birth (POB)\n",
    "for pob, bigrams in bigrams_cache_pob.items():\n",
    "    for bigram in bigrams:\n",
    "        bigram_counts_pob[bigram] += 1\n",
    "        bigram_groups_pob[bigram].add(pob)\n",
    "\n",
    "# Step 3: Filtering frequent bigrams\n",
    "total_names = len(unique_cleaned_names)\n",
    "bigram_frequency_threshold = total_names * 0.25 \n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pob_dict = defaultdict(list)\n",
    "\n",
    "# Iterate through the dataset\n",
    "for idx, row in dataset.iterrows():\n",
    "    # Get the cleaned_name and TEMPAT_LAHIR\n",
    "    name = row['cleaned_name']\n",
    "    pob = row['TEMPAT_LAHIR']\n",
    "    \n",
    "    # Skip rows with invalid names or places of birth\n",
    "    if not isinstance(name, str) or not name.strip() or not isinstance(pob, str) or not pob.strip():\n",
    "        continue\n",
    "    \n",
    "    pob_dict[pob].append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_bigrams_and_jaccard(name1, name2):\n",
    "    name1 = \"\".join(sorted(name1.replace(\" \", \"\")))  # Hilangkan spasi dan urutkan karakter\n",
    "    name2 = \"\".join(sorted(name2.replace(\" \", \"\")))  \n",
    "\n",
    "    set1, set2 = set(name1), set(name2)  # Buat himpunan karakter unik\n",
    "    intersection = len(set1 & set2)\n",
    "    union = len(set1 | set2)\n",
    "\n",
    "    similarity = intersection / union if union > 0 else 0  # Jaccard similarity\n",
    "    return similarity\n",
    "    \n",
    "# Function to find similar names based on POB bigrams\n",
    "def calculate_jaccard_distances(name, pob, similarity_list):\n",
    "    # Step 1: Get the top 3 least frequent bigrams from name and POB\n",
    "    bigrams_name = bigrams_cache_name[name]\n",
    "    bigrams_pob = bigrams_cache_pob[pob]\n",
    "\n",
    "    # Sort bigrams by frequency (lower frequency = more unique)\n",
    "    name_bigrams_sorted = sorted(bigrams_name, key=lambda x: bigram_counts_name[x])[:3]\n",
    "    pob_bigrams_sorted = sorted(bigrams_pob, key=lambda x: bigram_counts_pob[x])[:3]\n",
    "\n",
    "    list_of_names = set()\n",
    "    for bigram in name_bigrams_sorted:\n",
    "        list_of_names.update(bigram_groups_name[bigram])\n",
    "\n",
    "    # Step 2: Find matching names based on POB\n",
    "    matching_pobs = set()\n",
    "    for bigram in pob_bigrams_sorted:\n",
    "        matching_pobs.update(bigram_groups_pob[bigram])\n",
    "        \n",
    "    matching_name_pob_pairs = set()\n",
    "    for place in matching_pobs:\n",
    "        for matched_name in pob_dict[place]:  # Get names associated with this POB\n",
    "            if matched_name in list_of_names:\n",
    "                matching_name_pob_pairs.add((matched_name, place))\n",
    "\n",
    "    # Remove the original (name, pob) pair\n",
    "    matching_name_pob_pairs.discard((name, pob))\n",
    "\n",
    "    # Step 3: Compute Jaro-Winkler similarity for final filtered names\n",
    "    distances = {}\n",
    "    for other_name, other_pob in matching_name_pob_pairs:\n",
    "        similarity = compare_bigrams_and_jaccard(name, other_name)\n",
    "        if similarity and similarity > 0.75:\n",
    "            distances[(other_name, other_pob)] = similarity\n",
    "            similarity_list[name].append(similarity)\n",
    "    return name, distances\n",
    "\n",
    "def add_new_name_to_results_dict(new_name, new_pob, similarity_list):\n",
    "    # Generate bigrams for the new name\n",
    "    new_bigrams = get_bigrams(new_name)\n",
    "    new_bigrams_pob = get_bigrams(new_pob)\n",
    "    \n",
    "    # Add the new bigrams to the cache\n",
    "    bigrams_cache[new_name] = new_bigrams\n",
    "    bigrams_cache[new_pob] = new_bigrams_pob\n",
    "    \n",
    "    # Calculate Jaro-Winkler distances for the new name\n",
    "    name, distances = calculate_jaccard_distances(new_name, new_pob, similarity_list)\n",
    "\n",
    "    # If the new name has similar names, add it to results_list\n",
    "    if distances:\n",
    "        similar_names_df = pd.DataFrame(\n",
    "            list(distances.items()), columns=[\"similar_name_pob\", \"similarity\"]\n",
    "        )\n",
    "        results_list = similar_names_df[\"similar_name_pob\"].tolist()  # Convert similar names to a list\n",
    "    else:\n",
    "        results_list = []\n",
    "        \n",
    "    results_list.append((new_name, new_pob))\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 15698.31 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "final_blocks = defaultdict(list)\n",
    "similarity_list = defaultdict(list)\n",
    "\n",
    "for index, row in sampled_data_100000.iterrows():\n",
    "    final_blocks[row['cleaned_name'], row['TEMPAT_LAHIR']] = add_new_name_to_results_dict(\n",
    "        row['cleaned_name'], row['TEMPAT_LAHIR'], similarity_list\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair Completeness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_monre_0924 = pd.DataFrame(columns=['NO_AGGR', 'cleaned_name', 'SID'])\n",
    "mst_sid_0924 = pd.DataFrame(columns=['SID', 'cleaned_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  NAME ONLY\n",
    "monre_dict = defaultdict(list)\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    monre_dict[row['cleaned_name']].append(index)\n",
    "    \n",
    "# NAME ADDRESS\n",
    "monre_dict = defaultdict(list)\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    monre_dict[(row['cleaned_name'], row['deep_clean_address'])].append(index)\n",
    "\n",
    "# NAME TEMPAT LAHIR\n",
    "monre_dict = defaultdict(list)\n",
    "\n",
    "for index, row in dataset.iterrows():\n",
    "    monre_dict[(row['cleaned_name'], row['TEMPAT_LAHIR'])].append(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_distances(name):\n",
    "    bigrams = bigrams_cache[name]  # Use precomputed bigrams\n",
    "    bigram_weights = [\n",
    "        (bigram, bigram_counts[bigram]) for bigram in bigrams if bigram in bigram_groups\n",
    "        and bigram_counts[bigram] <= bigram_frequency_threshold  # Filter bigrams based on frequency\n",
    "    ]\n",
    "\n",
    "    # Sort bigrams by their frequency (lower frequency = higher weight)\n",
    "    bigram_weights_sorted = sorted(bigram_weights, key=lambda x: x[1])\n",
    "\n",
    "    # Select the top 3 least frequent bigrams\n",
    "    selected_bigrams = [bg[0] for bg in bigram_weights_sorted[:3]]\n",
    "\n",
    "    # Find all unique names in the groups of these 3 bigrams\n",
    "    matching_names = set()\n",
    "    for bigram in selected_bigrams:\n",
    "        matching_names.update(bigram_groups[bigram])\n",
    "\n",
    "    matching_names.discard(name)  # Remove the original name from comparison\n",
    "    \n",
    "    # Compute Jaro-Winkler similarity for each name in the matching group using bigram comparison\n",
    "    distances = {}\n",
    "    \n",
    "    for other_name in matching_names:\n",
    "        # Use the new comparison function to check bigram overlap and compute Jaro-Winkler\n",
    "        similarity = compare_bigrams_and_jaccard(name, other_name)\n",
    "        if similarity and similarity > 0.75:\n",
    "            distances[other_name] = similarity\n",
    "\n",
    "    return name, distances  # Return the name and its distances\n",
    "\n",
    "def add_new_name_to_results_dict(new_name):\n",
    "    # Generate bigrams for the new name\n",
    "    new_bigrams = get_bigrams(new_name)\n",
    "    \n",
    "    # Add the new bigrams to the cache\n",
    "    bigrams_cache[new_name] = new_bigrams\n",
    "    \n",
    "    # Calculate Jaro-Winkler distances for the new name\n",
    "    name, distances = calculate_jaccard_distances(new_name)\n",
    "    \n",
    "    # If the new name has similar names, add it to results_list\n",
    "    if distances:\n",
    "        similar_names_df = pd.DataFrame(\n",
    "            list(distances.items()), columns=[\"similar_name\", \"similarity\"]\n",
    "        )\n",
    "        results_list = similar_names_df[\"similar_name\"].tolist()  # Convert similar names to a list\n",
    "    else:\n",
    "        results_list = []\n",
    "    results_list.append(new_name)\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name & Address"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find similar names based on address bigrams\n",
    "def calculate_levenshtein_distances(name, address):\n",
    "    # Step 1: Get the top 3 least frequent bigrams from name and address\n",
    "    bigrams_name = bigrams_cache_name[name]\n",
    "    bigrams_address = bigrams_cache_address[address]\n",
    "\n",
    "    # Sort bigrams by frequency (lower frequency = more unique)\n",
    "    name_bigrams_sorted = sorted(bigrams_name, key=lambda x: bigram_counts_name[x])[:3]\n",
    "    address_bigrams_sorted = sorted(bigrams_address, key=lambda x: bigram_counts_address[x])[:3]\n",
    "\n",
    "    list_of_names = set()\n",
    "    for bigram in name_bigrams_sorted:\n",
    "        list_of_names.update(bigram_groups_name[bigram])\n",
    "\n",
    "    # Step 2: Find matching names based on address\n",
    "    matching_addresses = set()\n",
    "    for bigram in address_bigrams_sorted:\n",
    "        matching_addresses.update(bigram_groups_address[bigram])\n",
    "        \n",
    "    matching_name_address_pairs = set()\n",
    "    for addr in matching_addresses:\n",
    "        for matched_name in address_dict[addr]:  # Get names associated with this address\n",
    "            if matched_name in list_of_names:\n",
    "                matching_name_address_pairs.add((matched_name, addr))\n",
    "\n",
    "    # Remove the original (name, address) pair\n",
    "    matching_name_address_pairs.discard((name, address))\n",
    "    # matching_names.intersection_update(list_of_names)\n",
    "    \n",
    "    # Step 3: Compute Jaro-Winkler similarity for final filtered names\n",
    "    distances = {}\n",
    "    for other_name, other_address in matching_name_address_pairs:\n",
    "        similarity = compare_bigrams_and_jaccard(name, other_name)\n",
    "        if similarity and similarity > 0.75:\n",
    "            distances[(other_name,other_address)] = similarity\n",
    "\n",
    "    return name, distances\n",
    "\n",
    "def add_new_name_to_results_dict(new_name, new_address):\n",
    "    # Generate bigrams for the new name\n",
    "    new_bigrams = get_bigrams(new_name)\n",
    "    new_bigrams_address = get_bigrams(new_address)\n",
    "    \n",
    "    # Add the new bigrams to the cache\n",
    "    bigrams_cache_name[new_name] = new_bigrams\n",
    "    bigrams_cache_address[new_address] = new_bigrams_address\n",
    "    \n",
    "    # Calculate Jaro-Winkler distances for the new name\n",
    "    name, distances = calculate_levenshtein_distances(new_name, new_address)\n",
    "    \n",
    "    # If the new name has similar names, add it to results_list\n",
    "    if distances:\n",
    "        similar_names_df = pd.DataFrame(\n",
    "            list(distances.items()), columns=[\"similar_name_address\", \"similarity\"]\n",
    "        )\n",
    "        results_list = similar_names_df[\"similar_name_address\"].tolist()  # Convert similar names to a list\n",
    "    else:\n",
    "        results_list = []\n",
    "    results_list.append((new_name,new_address))\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAME & TEMPAT LAHIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JACCARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find similar names based on POB bigrams\n",
    "def calculate_jaccard_distances(name, pob):\n",
    "    # Step 1: Get the top 3 least frequent bigrams from name and POB\n",
    "    bigrams_name = bigrams_cache_name[name]\n",
    "    bigrams_pob = bigrams_cache_pob[pob]\n",
    "\n",
    "    # Sort bigrams by frequency (lower frequency = more unique)\n",
    "    name_bigrams_sorted = sorted(bigrams_name, key=lambda x: bigram_counts_name[x])[:3]\n",
    "    pob_bigrams_sorted = sorted(bigrams_pob, key=lambda x: bigram_counts_pob[x])[:3]\n",
    "\n",
    "    list_of_names = set()\n",
    "    for bigram in name_bigrams_sorted:\n",
    "        list_of_names.update(bigram_groups_name[bigram])\n",
    "\n",
    "    # Step 2: Find matching names based on POB\n",
    "    matching_pobs = set()\n",
    "    for bigram in pob_bigrams_sorted:\n",
    "        matching_pobs.update(bigram_groups_pob[bigram])\n",
    "        \n",
    "    matching_name_pob_pairs = set()\n",
    "    for place in matching_pobs:\n",
    "        for matched_name in pob_dict[place]:  # Get names associated with this POB\n",
    "            if matched_name in list_of_names:\n",
    "                matching_name_pob_pairs.add((matched_name, place))\n",
    "\n",
    "    # Remove the original (name, pob) pair\n",
    "    matching_name_pob_pairs.discard((name, pob))\n",
    "\n",
    "    # Step 3: Compute Jaro-Winkler similarity for final filtered names\n",
    "    distances = {}\n",
    "    for other_name, other_pob in matching_name_pob_pairs:\n",
    "        similarity = compare_bigrams_and_jaccard(name, other_name)\n",
    "        if similarity and similarity > 0.75:\n",
    "            distances[(other_name, other_pob)] = similarity\n",
    "    return name, distances\n",
    "\n",
    "def add_new_name_to_results_dict(new_name, new_pob):\n",
    "    # Generate bigrams for the new name\n",
    "    new_bigrams = get_bigrams(new_name)\n",
    "    new_bigrams_pob = get_bigrams(new_pob)\n",
    "    \n",
    "    # Add the new bigrams to the cache\n",
    "    bigrams_cache[new_name] = new_bigrams\n",
    "    bigrams_cache[new_pob] = new_bigrams_pob\n",
    "    \n",
    "    # Calculate Jaro-Winkler distances for the new name\n",
    "    name, distances = calculate_jaccard_distances(new_name, new_pob)\n",
    "\n",
    "    # If the new name has similar names, add it to results_list\n",
    "    if distances:\n",
    "        similar_names_df = pd.DataFrame(\n",
    "            list(distances.items()), columns=[\"similar_name_pob\", \"similarity\"]\n",
    "        )\n",
    "        results_list = similar_names_df[\"similar_name_pob\"].tolist()  # Convert similar names to a list\n",
    "    else:\n",
    "        results_list = []\n",
    "        \n",
    "    results_list.append((new_name, new_pob))\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaro_winkler_match(value1, value2, threshold=0.92):\n",
    "    if pd.notna(value1) and pd.notna(value2):\n",
    "        value1_str = str(value1).strip() \n",
    "        value2_str = str(value2).strip() \n",
    "        \n",
    "        # Ensure neither value is an empty string\n",
    "        if value1_str and value2_str:\n",
    "            similarity = distance.get_jaro_distance(value1_str, value2_str)\n",
    "            return similarity >= threshold\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME ONLY\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "no_aggr_set = set(sid_monre_0924['NO_AGGR'])\n",
    "\n",
    "for index, row in sampled_data_100000.iterrows():\n",
    "    if row['NO_AGGR'] not in no_aggr_set:\n",
    "        row_compared = row\n",
    "        matched_dfs = [] \n",
    "        results_list = add_new_name_to_results_dict(row['cleaned_name'])\n",
    "        for name in results_list:\n",
    "            if name in monre_dict:\n",
    "                indices = monre_dict[name]\n",
    "                matched_dfs.append(dataset.loc[indices])  \n",
    "\n",
    "        if matched_dfs:\n",
    "            matched_df = pd.concat(matched_dfs, ignore_index=True)\n",
    "        else:\n",
    "            matched_df = pd.DataFrame()\n",
    "\n",
    "        if not matched_df.empty:\n",
    "            result_df_nodup = matched_df.drop_duplicates(subset='NO_AGGR').reset_index(drop=True)\n",
    "        else:\n",
    "            result_df_nodup = pd.DataFrame()\n",
    "\n",
    "        time_start = time.time()\n",
    "        aggr_compared = row['NO_AGGR']\n",
    "        name_compared = row['cleaned_name']\n",
    "        dob_compared = row['TGL_LAHIR']\n",
    "        tempat_compared = row['cleaned_TEMPAT_LAHIR']\n",
    "        ktp_kitas_compared = row['cleaned_no_ktp']\n",
    "        mother_name_compared = row['cleaned_NAMA_IBU_KANDUNG'] \n",
    "        npwp_compared = row['cleaned_no_npwp']\n",
    "        address_compared = row['cleaned_alamat']\n",
    "\n",
    "        # Filter result_df based on matching criteria\n",
    "        filtered_result_df = result_df_nodup[\n",
    "            ((pd.notna(result_df_nodup['TGL_LAHIR']) & pd.notna(dob_compared) & \n",
    "            (result_df_nodup['TGL_LAHIR'] == dob_compared)) |\n",
    "            (pd.notna(result_df_nodup['cleaned_no_ktp']) & pd.notna(ktp_kitas_compared) & \n",
    "            (result_df_nodup['cleaned_no_ktp'] == ktp_kitas_compared)) |\n",
    "            (pd.notna(result_df_nodup['cleaned_NAMA_IBU_KANDUNG']) & pd.notna(mother_name_compared) & \n",
    "            (result_df_nodup['cleaned_NAMA_IBU_KANDUNG'] == mother_name_compared)) |\n",
    "            (pd.notna(result_df_nodup['cleaned_no_npwp']) & pd.notna(npwp_compared) & \n",
    "            (result_df_nodup['cleaned_no_npwp'] == npwp_compared))\n",
    "        )]\n",
    "        \n",
    "        filtered_result_df = filtered_result_df.copy()\n",
    "        filtered_result_df['flag_SID'] = 'N'  \n",
    "        filtered_result_df['rule_num'] = None\n",
    "\n",
    "        for index, row in filtered_result_df.iterrows():\n",
    "            name_sim = jaro_winkler_match(row['cleaned_name'], name_compared)\n",
    "            \n",
    "            if (\n",
    "                name_sim and \n",
    "                pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared and \n",
    "                pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  # Rule 1\n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 1'\n",
    "                # print('MATCHING RULE 1 APPLIED')\n",
    "\n",
    "            elif (\n",
    "                name_sim and \n",
    "                pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 2\n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 2'\n",
    "                # print('MATCHING RULE 2 APPLIED')\n",
    "\n",
    "            elif (\n",
    "                name_sim and \n",
    "                pd.notna(row['cleaned_no_npwp']) and row['cleaned_no_npwp'] == npwp_compared  # Rule 3\n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 3'\n",
    "                # print('MATCHING RULE 3 APPLIED')\n",
    "\n",
    "            elif (\n",
    "                jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and  # Rule 4\n",
    "                pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  \n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 4' \n",
    "                # print('MATCHING RULE 4 APPLIED')\n",
    "\n",
    "            elif (\n",
    "                pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 5\n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 5'  \n",
    "                # print('MATCHING RULE 5 APPLIED')\n",
    "            \n",
    "            elif (\n",
    "                jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and\n",
    "                pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared # Rule 6\n",
    "            ):\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = 'RULE 6' \n",
    "                # print('MATCHING RULE 6 APPLIED')\n",
    "\n",
    "            else:\n",
    "                filtered_result_df.loc[index, 'flag_SID'] = 'N'\n",
    "                filtered_result_df.loc[index, 'rule_num'] = None \n",
    "\n",
    "        filtered_result_df = filtered_result_df[filtered_result_df['flag_SID'] == 'Y'].reset_index(drop=True)\n",
    "        \n",
    "        if filtered_result_df.empty:\n",
    "            row_compared_df = pd.DataFrame([row_compared])  \n",
    "            row_compared_df['flag_SID'] = 'Y'              \n",
    "            filtered_result_df = pd.concat([filtered_result_df, row_compared_df], ignore_index=True)\n",
    "\n",
    "        check_sid_exist = False\n",
    "        for no_aggr in filtered_result_df['NO_AGGR']:\n",
    "            if no_aggr in no_aggr_set:\n",
    "                check_sid_exist = True\n",
    "                filtered_result_df['SID'] = sid_monre_0924.loc[sid_monre_0924['NO_AGGR'] == no_aggr, 'SID'].values[0]\n",
    "                break\n",
    "            \n",
    "        if check_sid_exist:\n",
    "            count += 1\n",
    "\n",
    "        if not check_sid_exist:\n",
    "            filtered_result_df = filtered_result_df.sort_values(by='DT_GOLIVE_VALID').reset_index(drop=True)\n",
    "            \n",
    "            kode_cabang = str(filtered_result_df.loc[0, 'CD_SP'])\n",
    "            \n",
    "            matching_sid_rows = mst_sid_0924[mst_sid_0924['SID'].str[:6] == kode_cabang]\n",
    "            last_sequence = matching_sid_rows['SID'].str[-7:].astype(int).max() if not matching_sid_rows.empty else 0\n",
    "            \n",
    "            filtered_result_df['SID'] = f\"{kode_cabang}{(last_sequence + 1):07d}\"\n",
    "\n",
    "        rows_to_append_filtered = filtered_result_df[~filtered_result_df['NO_AGGR'].isin(no_aggr_set)]\n",
    "\n",
    "        if not rows_to_append_filtered.empty:  \n",
    "            sid_monre_0924 = pd.concat([sid_monre_0924, rows_to_append_filtered[['NO_AGGR', 'cleaned_name', 'SID', 'rule_num']]], ignore_index=True)\n",
    "            no_aggr_set.update(rows_to_append_filtered['NO_AGGR'].tolist())\n",
    "            if not check_sid_exist:\n",
    "                rows_to_append_2 = rows_to_append_filtered.iloc[[0]][['cleaned_name', 'SID']]\n",
    "                mst_sid_0924 = pd.concat([mst_sid_0924, rows_to_append_2], ignore_index=True)\n",
    "\n",
    "mst_sid_0924.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 6.04 seconds\n"
     ]
    }
   ],
   "source": [
    "# NAME & ADDRESS\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "no_aggr_set = set(sid_monre_0924['NO_AGGR'])\n",
    "\n",
    "for index, row in sampled_data_100000.head(5).iterrows():\n",
    "    if row['NO_AGGR'] not in no_aggr_set:\n",
    "        row_compared = row\n",
    "        matched_dfs = []\n",
    "        if row['deep_clean_address'] and row['deep_clean_address'] != 'nan':\n",
    "            results_list = add_new_name_to_results_dict(row['cleaned_name'], row['deep_clean_address'])\n",
    "            for tuple in results_list:\n",
    "                if tuple in monre_dict:\n",
    "                    indices = monre_dict[tuple]\n",
    "                    matched_dfs.append(dataset.loc[indices])  \n",
    "\n",
    "            if matched_dfs:\n",
    "                matched_df = pd.concat(matched_dfs, ignore_index=True)\n",
    "                result_df_nodup = matched_df.drop_duplicates(subset='NO_AGGR').reset_index(drop=True)\n",
    "            # else:\n",
    "            #     result_df_nodup = pd.DataFrame()\n",
    "            #     print(row['cleaned_name'])\n",
    "            #     print(row['deep_clean_address'])\n",
    "\n",
    "                aggr_compared = row['NO_AGGR']\n",
    "                name_compared = row['cleaned_name']\n",
    "                dob_compared = row['TGL_LAHIR']\n",
    "                tempat_compared = row['cleaned_TEMPAT_LAHIR']\n",
    "                ktp_kitas_compared = row['cleaned_no_ktp']\n",
    "                mother_name_compared = row['cleaned_NAMA_IBU_KANDUNG'] \n",
    "                npwp_compared = row['cleaned_no_npwp']\n",
    "                address_compared = row['cleaned_alamat']\n",
    "\n",
    "                # Filter result_df based on matching criteria\n",
    "                filtered_result_df = result_df_nodup[\n",
    "                    ((pd.notna(result_df_nodup['TGL_LAHIR']) & pd.notna(dob_compared) & \n",
    "                    (result_df_nodup['TGL_LAHIR'] == dob_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_no_ktp']) & pd.notna(ktp_kitas_compared) & \n",
    "                    (result_df_nodup['cleaned_no_ktp'] == ktp_kitas_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_NAMA_IBU_KANDUNG']) & pd.notna(mother_name_compared) & \n",
    "                    (result_df_nodup['cleaned_NAMA_IBU_KANDUNG'] == mother_name_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_no_npwp']) & pd.notna(npwp_compared) & \n",
    "                    (result_df_nodup['cleaned_no_npwp'] == npwp_compared))\n",
    "                )]\n",
    "                \n",
    "                filtered_result_df = filtered_result_df.copy()\n",
    "                filtered_result_df['flag_SID'] = 'N'  \n",
    "                filtered_result_df['rule_num'] = None\n",
    "\n",
    "                for index, row in filtered_result_df.iterrows():\n",
    "                    name_sim = jaro_winkler_match(row['cleaned_name'], name_compared)\n",
    "                    \n",
    "                    if (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared and \n",
    "                        pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  # Rule 1\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 1'\n",
    "                        # print('MATCHING RULE 1 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 2\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 2'\n",
    "                        # print('MATCHING RULE 2 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['cleaned_no_npwp']) and row['cleaned_no_npwp'] == npwp_compared  # Rule 3\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 3'\n",
    "                        # print('MATCHING RULE 3 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                        pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and  # Rule 4\n",
    "                        pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  \n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 4' \n",
    "                        # print('MATCHING RULE 4 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 5\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 5'  \n",
    "                        # print('MATCHING RULE 5 APPLIED')\n",
    "                    \n",
    "                    elif (\n",
    "                        jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                        pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and\n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared # Rule 6\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 6' \n",
    "                        # print('MATCHING RULE 6 APPLIED')\n",
    "\n",
    "                    else:\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'N'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = None \n",
    "\n",
    "                filtered_result_df = filtered_result_df[filtered_result_df['flag_SID'] == 'Y'].reset_index(drop=True)\n",
    "                \n",
    "                if filtered_result_df.empty:\n",
    "                    row_compared_df = pd.DataFrame([row_compared])  \n",
    "                    row_compared_df['flag_SID'] = 'Y'              \n",
    "                    filtered_result_df = pd.concat([filtered_result_df, row_compared_df], ignore_index=True)\n",
    "\n",
    "                check_sid_exist = False\n",
    "                for no_aggr in filtered_result_df['NO_AGGR']:\n",
    "                    if no_aggr in no_aggr_set:\n",
    "                        check_sid_exist = True\n",
    "                        filtered_result_df['SID'] = sid_monre_0924.loc[sid_monre_0924['NO_AGGR'] == no_aggr, 'SID'].values[0]\n",
    "                        break\n",
    "                    \n",
    "                if check_sid_exist:\n",
    "                    count += 1\n",
    "\n",
    "                if not check_sid_exist:\n",
    "                    filtered_result_df = filtered_result_df.sort_values(by='DT_GOLIVE_VALID').reset_index(drop=True)\n",
    "                    \n",
    "                    kode_cabang = str(filtered_result_df.loc[0, 'CD_SP'])\n",
    "                    \n",
    "                    matching_sid_rows = mst_sid_0924[mst_sid_0924['SID'].str[:6] == kode_cabang]\n",
    "                    last_sequence = matching_sid_rows['SID'].str[-7:].astype(int).max() if not matching_sid_rows.empty else 0\n",
    "                    \n",
    "                    filtered_result_df['SID'] = f\"{kode_cabang}{(last_sequence + 1):07d}\"\n",
    "\n",
    "                rows_to_append_filtered = filtered_result_df[~filtered_result_df['NO_AGGR'].isin(no_aggr_set)]\n",
    "\n",
    "                if not rows_to_append_filtered.empty:  \n",
    "                    sid_monre_0924 = pd.concat([sid_monre_0924, rows_to_append_filtered[['NO_AGGR', 'cleaned_name', 'SID', 'rule_num']]], ignore_index=True)\n",
    "                    no_aggr_set.update(rows_to_append_filtered['NO_AGGR'].tolist())\n",
    "                    if not check_sid_exist:\n",
    "                        rows_to_append_2 = rows_to_append_filtered.iloc[[0]][['cleaned_name', 'SID']]\n",
    "                        mst_sid_0924 = pd.concat([mst_sid_0924, rows_to_append_2], ignore_index=True)\n",
    "\n",
    "mst_sid_0924.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME & TEMPAT LAHIR\n",
    "start_time = time.time()\n",
    "count = 0\n",
    "no_aggr_set = set(sid_monre_0924['NO_AGGR'])\n",
    "\n",
    "for index, row in sampled_data_100000.head(5).iterrows():\n",
    "    if row['NO_AGGR'] not in no_aggr_set:\n",
    "        row_compared = row\n",
    "        matched_dfs = []\n",
    "        if row['TEMPAT_LAHIR'] and row['TEMPAT_LAHIR'] != 'nan':\n",
    "            results_list = add_new_name_to_results_dict(row['cleaned_name'], row['TEMPAT_LAHIR'])\n",
    "            for tuple in results_list:\n",
    "                if tuple in monre_dict:\n",
    "                    indices = monre_dict[tuple]\n",
    "                    matched_dfs.append(dataset.loc[indices])  \n",
    "\n",
    "            if matched_dfs:\n",
    "                matched_df = pd.concat(matched_dfs, ignore_index=True)\n",
    "                result_df_nodup = matched_df.drop_duplicates(subset='NO_AGGR').reset_index(drop=True)\n",
    "            # else:\n",
    "            #     result_df_nodup = pd.DataFrame()\n",
    "            #     print(row['cleaned_name'])\n",
    "            #     print(row['deep_clean_address'])\n",
    "\n",
    "                aggr_compared = row['NO_AGGR']\n",
    "                name_compared = row['cleaned_name']\n",
    "                dob_compared = row['TGL_LAHIR']\n",
    "                tempat_compared = row['cleaned_TEMPAT_LAHIR']\n",
    "                ktp_kitas_compared = row['cleaned_no_ktp']\n",
    "                mother_name_compared = row['cleaned_NAMA_IBU_KANDUNG'] \n",
    "                npwp_compared = row['cleaned_no_npwp']\n",
    "                address_compared = row['cleaned_alamat']\n",
    "\n",
    "                # Filter result_df based on matching criteria\n",
    "                filtered_result_df = result_df_nodup[\n",
    "                    ((pd.notna(result_df_nodup['TGL_LAHIR']) & pd.notna(dob_compared) & \n",
    "                    (result_df_nodup['TGL_LAHIR'] == dob_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_no_ktp']) & pd.notna(ktp_kitas_compared) & \n",
    "                    (result_df_nodup['cleaned_no_ktp'] == ktp_kitas_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_NAMA_IBU_KANDUNG']) & pd.notna(mother_name_compared) & \n",
    "                    (result_df_nodup['cleaned_NAMA_IBU_KANDUNG'] == mother_name_compared)) |\n",
    "                    (pd.notna(result_df_nodup['cleaned_no_npwp']) & pd.notna(npwp_compared) & \n",
    "                    (result_df_nodup['cleaned_no_npwp'] == npwp_compared))\n",
    "                )]\n",
    "                \n",
    "                filtered_result_df = filtered_result_df.copy()\n",
    "                filtered_result_df['flag_SID'] = 'N'  \n",
    "                filtered_result_df['rule_num'] = None\n",
    "\n",
    "                for index, row in filtered_result_df.iterrows():\n",
    "                    name_sim = jaro_winkler_match(row['cleaned_name'], name_compared)\n",
    "                    \n",
    "                    if (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared and \n",
    "                        pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  # Rule 1\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 1'\n",
    "                        # print('MATCHING RULE 1 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 2\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 2'\n",
    "                        # print('MATCHING RULE 2 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        name_sim and \n",
    "                        pd.notna(row['cleaned_no_npwp']) and row['cleaned_no_npwp'] == npwp_compared  # Rule 3\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 3'\n",
    "                        # print('MATCHING RULE 3 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                        pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and  # Rule 4\n",
    "                        pd.notna(row['cleaned_NAMA_IBU_KANDUNG']) and jaro_winkler_match(row['cleaned_NAMA_IBU_KANDUNG'], mother_name_compared)  \n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 4' \n",
    "                        # print('MATCHING RULE 4 APPLIED')\n",
    "\n",
    "                    elif (\n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_no_ktp']) and row['cleaned_no_ktp'] == ktp_kitas_compared  # Rule 5\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 5'  \n",
    "                        # print('MATCHING RULE 5 APPLIED')\n",
    "                    \n",
    "                    elif (\n",
    "                        jaro_winkler_match(row['cleaned_name'], name_compared, threshold=0.95) and \n",
    "                        pd.notna(row['cleaned_alamat']) and jaro_winkler_match(row['cleaned_alamat'], address_compared) and\n",
    "                        pd.notna(row['TGL_LAHIR']) and row['TGL_LAHIR'] == dob_compared and \n",
    "                        pd.notna(row['cleaned_TEMPAT_LAHIR']) and row['cleaned_TEMPAT_LAHIR'] == tempat_compared # Rule 6\n",
    "                    ):\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'Y'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = 'RULE 6' \n",
    "                        # print('MATCHING RULE 6 APPLIED')\n",
    "\n",
    "                    else:\n",
    "                        filtered_result_df.loc[index, 'flag_SID'] = 'N'\n",
    "                        filtered_result_df.loc[index, 'rule_num'] = None \n",
    "\n",
    "                filtered_result_df = filtered_result_df[filtered_result_df['flag_SID'] == 'Y'].reset_index(drop=True)\n",
    "                \n",
    "                if filtered_result_df.empty:\n",
    "                    row_compared_df = pd.DataFrame([row_compared])  \n",
    "                    row_compared_df['flag_SID'] = 'Y'              \n",
    "                    filtered_result_df = pd.concat([filtered_result_df, row_compared_df], ignore_index=True)\n",
    "\n",
    "                check_sid_exist = False\n",
    "                for no_aggr in filtered_result_df['NO_AGGR']:\n",
    "                    if no_aggr in no_aggr_set:\n",
    "                        check_sid_exist = True\n",
    "                        filtered_result_df['SID'] = sid_monre_0924.loc[sid_monre_0924['NO_AGGR'] == no_aggr, 'SID'].values[0]\n",
    "                        break\n",
    "                    \n",
    "                if check_sid_exist:\n",
    "                    count += 1\n",
    "\n",
    "                if not check_sid_exist:\n",
    "                    filtered_result_df = filtered_result_df.sort_values(by='DT_GOLIVE_VALID').reset_index(drop=True)\n",
    "                    \n",
    "                    kode_cabang = str(filtered_result_df.loc[0, 'CD_SP'])\n",
    "                    \n",
    "                    matching_sid_rows = mst_sid_0924[mst_sid_0924['SID'].str[:6] == kode_cabang]\n",
    "                    last_sequence = matching_sid_rows['SID'].str[-7:].astype(int).max() if not matching_sid_rows.empty else 0\n",
    "                    \n",
    "                    filtered_result_df['SID'] = f\"{kode_cabang}{(last_sequence + 1):07d}\"\n",
    "\n",
    "                rows_to_append_filtered = filtered_result_df[~filtered_result_df['NO_AGGR'].isin(no_aggr_set)]\n",
    "\n",
    "                if not rows_to_append_filtered.empty:  \n",
    "                    sid_monre_0924 = pd.concat([sid_monre_0924, rows_to_append_filtered[['NO_AGGR', 'cleaned_name', 'SID', 'rule_num']]], ignore_index=True)\n",
    "                    no_aggr_set.update(rows_to_append_filtered['NO_AGGR'].tolist())\n",
    "                    if not check_sid_exist:\n",
    "                        rows_to_append_2 = rows_to_append_filtered.iloc[[0]][['cleaned_name', 'SID']]\n",
    "                        mst_sid_0924 = pd.concat([mst_sid_0924, rows_to_append_2], ignore_index=True)\n",
    "\n",
    "mst_sid_0924.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# End measuring time\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the elapsed time\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mst_sid_0924.to_excel(r\"D:\\Kuliah\\SEMESTER 8\\Skripsi\\Pair Completion\\mst_sid_bigram_jaccard.xlsx\", index=False)\n",
    "sid_monre_0924.to_excel(r\"D:\\Kuliah\\SEMESTER 8\\Skripsi\\Pair Completion\\sid_monre_bigram_jaccard.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction Ratio: 0.9999\n",
      "13808640\n"
     ]
    }
   ],
   "source": [
    "n = len(dataset) \n",
    "m = len(sampled_data_100000) \n",
    "total_comparisons_before = n * m\n",
    "\n",
    "# Calculate Total Comparisons After Blocking\n",
    "comparisons_after_blocking = 0\n",
    "for block in final_blocks.values():\n",
    "    comparisons_after_blocking += len(block)\n",
    "\n",
    "# Calculate Reduction Ratio\n",
    "reduction_ratio = 1 - (comparisons_after_blocking / total_comparisons_before)\n",
    "print(f\"Reduction Ratio: {reduction_ratio:.4f}\")\n",
    "print(comparisons_after_blocking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Lenght : 2095454 Data\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Lenght :\", len(dataset), \"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Lenght : 100000 Data\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Lenght :\", len(sampled_data_100000), \"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Distance :  0.8197966792492957\n"
     ]
    }
   ],
   "source": [
    "avg_distance = []\n",
    "for compared_name, distances in similarity_list.items():\n",
    "    avg_distance.append(np.mean(distances))\n",
    "\n",
    "# Calculate the overall average\n",
    "overall_avg_distance = np.mean(avg_distance)\n",
    "print('Average Distance : ',overall_avg_distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
